{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`{r setup, message=FALSE,warning=FALSE} library(lubridate); library(ggplot2); library(tidyverse); library(rmarkdown); library(rpact); library(xgboost); library(magrittr); library(skimr); library(patchwork); library(lattice); library(scales); library(corrplot); library(data.table); library(e1071); library(DT);library(plotly); library(VIM); library(caret); library(glmnet); library(pROC); library(dplyr); library(mice); library(ggcorrplot);library(broom);library(kableExtra);library(lightgbm); library(tictoc);library(zoo);library(Boruta);library(doParallel);library(rpart.plot);library(rpart);library(writexl) #seed for the entire document seed<-500`\n",
    "\n",
    "### 1. Introduction\n",
    "\n",
    "Access to credit is crucial for development globally, enabling\n",
    "individuals and businesses to leverage financial resources for growth.\n",
    "However, many people face barriers to credit access due to inadequate\n",
    "information, such as risk agency scores, making them susceptible to\n",
    "predatory lending and severe financial consequences. Home Credit seeks\n",
    "to address this issue by providing secure credit access to underserved\n",
    "populations. This project aims to develop a default prediction model\n",
    "using advanced analytics, aligning with Home Credit’s goal of fostering\n",
    "financial inclusion while ensuring the company’s financial stability.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### 2. Data set\n",
    "\n",
    "***Data set structure and description***\n",
    "\n",
    "\\`\\`\\`{r Loading dataset} \\# Set the working directory mydir \\<- getwd()\n",
    "setwd(mydir)\n",
    "\n",
    "# Loading train data\n",
    "\n",
    "application_train \\<- read.csv(file =\n",
    "“C:\\Home_Credit_Project\\Home-Credit-Project\\home-credit-default-risk\\application_train.csv”,\n",
    "stringsAsFactors = FALSE)\n",
    "\n",
    "# Loading test data\n",
    "\n",
    "application_test \\<- read.csv(file =\n",
    "“C:\\Home_Credit_Project\\Home-Credit-Project\\home-credit-default-risk\\application_test.csv”,\n",
    "stringsAsFactors = FALSE)\n",
    "\n",
    "# Display the first 6 columns of the dataset\n",
    "\n",
    "application_train %\\>% head() %\\>% knitr::kable()\n",
    "\n",
    "\n",
    "    ```{r}\n",
    "    # Number of variables and types\n",
    "    variable_types <- sapply(application_train, class)\n",
    "    type_counts <- table(variable_types)\n",
    "\n",
    "    knitr::kable(type_counts)\n",
    "\n",
    "This data set contains information about consumers who have obtained\n",
    "credit from Home Credit. It includes demographic and business details to\n",
    "support the credit granting process.\n",
    "\n",
    "The application_train table will serve as the starting point for\n",
    "creating predictive models. It comprises 122 columns and 307,511 rows,\n",
    "originally categorized into three types: character, numeric, and\n",
    "integer.\n",
    "\n",
    "The descriptions of each variable are detailed in a CSV file, which is\n",
    "quite extensive and, therefore, not included in this document. To access\n",
    "these descriptions, please visit:\n",
    "https://www.kaggle.com/c/home-credit-default-risk/data.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "To understand the data in the application_train table and to identify\n",
    "any potential errors or anomalies, a series of analyses will be\n",
    "conducted. Additionally, this phase aims to explore the characteristics\n",
    "and relationships between the target variable and other variables, which\n",
    "will help in developing a predictive model aligned with the company’s\n",
    "principles, strategies, and objectives.\n",
    "\n",
    "To make this document more understandable and straightforward, only the\n",
    "key analyses will be presented here. However, some code used for these\n",
    "analyses may be included even if their results are not displayed.\n",
    "\n",
    "#### 3.1 Target variable: Default\n",
    "\n",
    "The main objective of this project is to predict the binary outcome of\n",
    "the TARGET variable, which indicates loan default. The data set has a\n",
    "notable imbalance, with only about 8% of the observations corresponding\n",
    "to clients who are in default.\n",
    "\n",
    "``` {r}\n",
    "# Percentages for the target variable\n",
    "application_train %>%\n",
    "  group_by(TARGET) %>%\n",
    "  summarise(customers = n(),\n",
    "            avg_loan = mean(AMT_CREDIT),\n",
    "            min = min(AMT_CREDIT),\n",
    "            max = max(AMT_CREDIT)) %>%\n",
    "  mutate(Percent. = round(((customers/sum(customers))*100),1))%>%\n",
    "  knitr::kable()\n",
    "```\n",
    "\n",
    "#### 3.2 Outliers and Anomalies\n",
    "\n",
    "#### Numeric Values\n",
    "\n",
    "In this analysis, extreme values will be examined to identify and\n",
    "address outliers that may clearly represent errors. Additionally, the\n",
    "logical integrity of the numeric values will be assessed to ensure\n",
    "consistency and accuracy within the data set.\n",
    "\n",
    "\\`\\`\\`{r, echo=FALSE, results=‘hide’} \\# Select only numeric variables\n",
    "numeric_vars \\<- application_train %\\>% select(where(is.numeric))\n",
    "\n",
    "# Generate summary for all numeric variables\n",
    "\n",
    "summary(numeric_vars)\n",
    "\n",
    "# Select only integer variables\n",
    "\n",
    "integer_vars \\<- application_train %\\>% select(where(is.integer))\n",
    "\n",
    "# Generate summary for all integer variables\n",
    "\n",
    "summary(integer_vars)\n",
    "\n",
    "\n",
    "    After reviewing the numeric and integer variables (results not shown), it was determined which variables are relevant to show their summary at this time due to anomalies in their values.\n",
    "\n",
    "    ```{r}\n",
    "    # Select specified columns\n",
    "    selected_columns <- application_train[, c(\"AMT_INCOME_TOTAL\", \n",
    "                                               \"AMT_CREDIT\", \n",
    "                                               \"AMT_ANNUITY\", \n",
    "                                               \"AMT_GOODS_PRICE\", \n",
    "                                               \"DAYS_BIRTH\", \n",
    "                                               \"DAYS_EMPLOYED\")]\n",
    "\n",
    "    # Generate statistical summary\n",
    "    summary(selected_columns)\n",
    "\n",
    "It is possible to observe extreme values and missing data (NAs) in the\n",
    "data set. The date-related variables are counted retroactively from the\n",
    "application date, so to accurately view the applicants’ ages, these\n",
    "variables will be transformed for better understanding.\n",
    "\n",
    "For AMT_ANNUITY, none of the 12 missing values are in the default group\n",
    "(target = 1), so I will impute them using the median of the observed\n",
    "values. Similarly, for AMT_GOODS_PRICE, with 278 missing values (21 in\n",
    "the default group), I will apply the same imputation method.\n",
    "\n",
    "#### Income\n",
    "\n",
    "For AMT_INCOME_TOTAL, five individuals reported incomes exceeding 5\n",
    "million, with one reporting 117 million. These extreme outliers are far\n",
    "from the variable’s median of \\$147,150 and are not the main focus of\n",
    "the company’s business plan. Additionally, 99% of the individuals have\n",
    "incomes lower than \\$500,000. Therefore, these five observations will be\n",
    "removed, and the transformation process will also be applied to the\n",
    "application_test data frame later.\n",
    "\n",
    "\\`\\`\\`{r transformation 1} \\# Create the clean data set app_train_clean\n",
    "\\<- application_train\n",
    "\n",
    "# Clean and transform\n",
    "\n",
    "app_train_clean \\<- app_train_clean %\\>% filter(AMT_INCOME_TOTAL \\<=\n",
    "5000000)\n",
    "\n",
    "# Income plot under 500K by default\n",
    "\n",
    "ggplot(subset(app_train_clean, AMT_INCOME_TOTAL \\<= 500000), aes(x =\n",
    "AMT_INCOME_TOTAL, fill = as.factor(TARGET))) + geom_histogram(alpha =\n",
    "0.6, bins = 30, position = “identity”) + labs(title = “Distribution of\n",
    "Total Income by Customer Defaultthan 500k - 99% of the customers”, x =\n",
    "“Total Income”, y = “Frequency”) + scale_x\\_continuous(labels =\n",
    "scales::label_number(scale = 1)) + scale_fill_manual(values = c(“khaki”,\n",
    "“darkseagreen”), name = “Customer Default”) + theme_minimal()+\n",
    "theme(plot.title = element_text(size = 10))\n",
    "\n",
    "\n",
    "    #### Customer age\n",
    "\n",
    "    Below, the DAYS_BIRTH variable will be transformed to provide a clearer understanding of the consumers' ages at the time of the loan application.\n",
    "    The data set app_train_clean will be created to store these transformations.\n",
    "\n",
    "    ```{r transformation 2}\n",
    "\n",
    "    # Transformation for better understanding ages\n",
    "    app_train_clean$DAYS_BIRTH <- app_train_clean$DAYS_BIRTH / -365\n",
    "\n",
    "    # Histogram of Ages by default\n",
    "    ggplot(app_train_clean, aes(x = DAYS_BIRTH, fill = as.factor(TARGET))) +\n",
    "      geom_histogram(alpha = 0.6, bins = 30, position = \"identity\") +\n",
    "      labs(title = \"Distribution of Days Birth by Customer Default\", \n",
    "           x = \"Days Birth\", \n",
    "           y = \"Frequency\") +\n",
    "      scale_fill_manual(values = c(\"khaki\", \"darkseagreen\"), name = \"Customer Default\") +\n",
    "      theme_minimal()+\n",
    "      theme(plot.title = element_text(size = 10))\n",
    "\n",
    "#### Days Employed (Years)\n",
    "\n",
    "The DAYS_EMPLOYED variable shows positive values, which should not\n",
    "occur.\n",
    "\n",
    "``` {r}\n",
    "# Visualize a table about the anomalies\n",
    "app_train_clean %>%\n",
    "  filter(DAYS_EMPLOYED > 0) %>%\n",
    "  group_by(TARGET) %>%\n",
    "  summarise(customers = n(),\n",
    "            min = min(DAYS_EMPLOYED),\n",
    "            max= max(DAYS_EMPLOYED)) %>%\n",
    "  mutate(Percent. = round((customers / sum(customers)) * 100, 1))\n",
    "```\n",
    "\n",
    "Since anomalous values account for a significant portion of the data set\n",
    "(55,374 entries), the strategy is to mark these entries as non-numeric\n",
    "and create a new column, DAYS_EMPLOYED_ANOM, to flag the presence of\n",
    "these anomalies for each customer.\n",
    "\n",
    "Below is the histogram of the corrected DAYS_EMPLOYED distribution. In\n",
    "this case, the distribution, transformed into years, is right-skewed.\n",
    "Most defaults are found among consumers with up to 5 years of\n",
    "employment.\n",
    "\n",
    "\\`\\`\\`{r transformations 3, message=FALSE,warning=FALSE} \\# Create a new\n",
    "column app_train_clean \\<- application_train %\\>%\n",
    "mutate(DAYS_EMPLOYED_ANOM = ifelse(DAYS_EMPLOYED == 365243, TRUE,\n",
    "FALSE))\n",
    "\n",
    "# Replace the anomalies for NA\n",
    "\n",
    "app_train_clean$DAYS_EMPLOYED[app_train_clean$DAYS_EMPLOYED == 365243\\]\n",
    "\\<- NA\n",
    "\n",
    "# Convert DAYS_EMPLOYED to years\n",
    "\n",
    "app_train_clean \\<- app_train_clean %\\>% mutate(DAYS_EMPLOYED_YEARS =\n",
    "DAYS_EMPLOYED / -365)\n",
    "\n",
    "# Remove DAYS_EMPLOYED\n",
    "\n",
    "app_train_clean \\<- app_train_clean %\\>% select(-DAYS_EMPLOYED)\n",
    "\n",
    "# Histogram of Days employed by default\n",
    "\n",
    "ggplot(app_train_clean, aes(x = DAYS_EMPLOYED_YEARS, fill =\n",
    "as.factor(TARGET))) + geom_histogram(alpha = 0.6, bins = 30, position =\n",
    "“identity”) + labs(title = “Distribution of Days Employed (in Years) by\n",
    "Customer Default”, x = “Years Employed”, y = “Frequency”) +\n",
    "scale_x\\_continuous(labels = scales::label_number(scale = 1)) +\n",
    "scale_fill_manual(values = c(“khaki”, “darkseagreen”), name = “Customer\n",
    "Default”) + theme_minimal()+ theme(plot.title = element_text(size = 10))\n",
    "\n",
    "\n",
    "    ------------------------------------------------------------------------\n",
    "\n",
    "    #### Character Variables\n",
    "\n",
    "    As seen earlier, there are 16 character variables in the data set.\n",
    "    The values of these variables will be converted to factors at this stage.\n",
    "    However, if the machine learning process requires other types of adjustments, these will be made later.\n",
    "    The code below was used to inspect these variables, but its results will not be shown as the key points will be discussed in the following sections.\n",
    "\n",
    "    ```{r echo=FALSE, results='hide'}\n",
    "    # Convert all character columns to factors\n",
    "    app_train_clean <- app_train_clean %>%\n",
    "      mutate(across(where(is.character), as.factor))\n",
    "\n",
    "    # Summary\n",
    "    summary(app_train_clean %>% select(where(is.factor)))\n",
    "\n",
    "For the variable CODE_GENDER, men represent 34% of the loans and have a\n",
    "default rate of 10%, which is 42% higher compared to women, who have a\n",
    "default rate of 7%.\n",
    "\n",
    "``` {r}\n",
    "# Get the total number of observations\n",
    "total_observations <- nrow(app_train_clean)\n",
    "\n",
    "# Percentage of customers by gender and default\n",
    "app_train_clean %>%\n",
    "  group_by(CODE_GENDER, TARGET) %>%\n",
    "  summarise(customers = n(), .groups = 'drop') %>%\n",
    "  group_by(CODE_GENDER) %>%\n",
    "  mutate(\n",
    "    Percent_of_total = round((customers / total_observations) * 100, 0),\n",
    "    Total_by_gender = sum(customers),\n",
    "    Percent_by_gender = round((customers / Total_by_gender) * 100, 0)\n",
    "  ) %>%\n",
    "  ungroup() %>%\n",
    "  knitr::kable()\n",
    "```\n",
    "\n",
    "The difference in default rates between consumers who own a car\n",
    "(FLAG_OWN_CAR) and those who do not is only 1.3% higher for those\n",
    "without a car. There is no difference in default rates between those who\n",
    "own property (FLAG_OWN_REALTY) and those who do not.\n",
    "\n",
    "While several variables can be converted to factors to differentiate\n",
    "between groups, they are not necessarily ranked by importance. Many of\n",
    "these variables contain blank values, which will be replaced with\n",
    "‘unknown’.\n",
    "\n",
    "\\`\\`\\`{r transformation 4, echo=FALSE, results=‘hide’} \\# Loop through\n",
    "each column of the dataset for (col in names(app_train_clean)) { \\#\n",
    "Check if the column is of character type if\n",
    "(is.character(app_train_clean\\[\\[col\\]\\])) { \\# Replace empty strings\n",
    "with “Unknown” app_train_clean\\[\\[col\\]\\] \\<-\n",
    "as.character(app_train_clean\\[\\[col\\]\\]) \\# Ensure column is character\n",
    "app_train_clean\\[\\[col\\]\\]\\[app_train_clean\\[\\[col\\]\\] == “”\\] \\<-\n",
    "“Unknown” \\# Replace empty strings app_train_clean\\[\\[col\\]\\] \\<-\n",
    "as.factor(app_train_clean\\[\\[col\\]\\]) \\# Convert back to factor if\n",
    "needed } }\n",
    "\n",
    "# Verify changes\n",
    "\n",
    "summary(app_train_clean)\n",
    "\n",
    "\n",
    "    #### Contract types\n",
    "\n",
    "    The vast majority (93.5%) of defaults are associated with cash loans, compared to just 6.5% for revolving loans.\n",
    "    Additionally, the average loan amount for cash loans is more than double at \\$578,598, with a maximum limit three times higher at \\$4,027,680.\n",
    "\n",
    "    ```{r}\n",
    "    # Loan types summary for default\n",
    "    app_train_clean %>%\n",
    "      filter(TARGET == \"1\") %>%\n",
    "      group_by(NAME_CONTRACT_TYPE) %>%\n",
    "      summarise(customers = n(),\n",
    "                avg_loan = mean(AMT_CREDIT),\n",
    "                min = min(AMT_CREDIT),\n",
    "                max= max(AMT_CREDIT)) %>%\n",
    "      mutate(Percent. = round((customers / sum(customers)) * 100, 1)) %>%\n",
    "      knitr::kable()\n",
    "\n",
    "Relatively, cash loans have a 50% higher default rate compared to\n",
    "revolving loans.\n",
    "\n",
    "``` {r}\n",
    "# Summarize data by contract type and default\n",
    "data_summary <- app_train_clean %>%\n",
    "  group_by(NAME_CONTRACT_TYPE, TARGET) %>%\n",
    "  summarize(count = n(), .groups = 'drop') %>%\n",
    "  group_by(NAME_CONTRACT_TYPE) %>%\n",
    "  mutate(total = sum(count),\n",
    "         percentage = (count / total) * 100) %>%\n",
    "  ungroup()\n",
    "\n",
    "# Extract percentage for TARGET == 0\n",
    "percentage_target_0 <- data_summary %>%\n",
    "  filter(TARGET == 0) %>%\n",
    "  select(NAME_CONTRACT_TYPE, percentage_target_0 = percentage)\n",
    "\n",
    "# Order and adjust data for plotting\n",
    "data_summary_ordered <- data_summary %>%\n",
    "  left_join(percentage_target_0, by = \"NAME_CONTRACT_TYPE\") %>%\n",
    "  mutate(order = if_else(is.na(percentage_target_0), 0, percentage_target_0)) %>%\n",
    "  arrange(desc(order)) %>%\n",
    "  mutate(NAME_CONTRACT_TYPE = factor(NAME_CONTRACT_TYPE, levels = unique(NAME_CONTRACT_TYPE)))\n",
    "\n",
    "# Contract type by default\n",
    "ggplot(data_summary_ordered, aes(x = percentage, y = reorder(NAME_CONTRACT_TYPE, -order), fill = as.factor(TARGET))) +\n",
    "  geom_bar(stat = \"identity\", position = \"stack\", alpha = 0.5) +  # Add transparency\n",
    "  geom_text(aes(label = scales::percent(percentage / 100, accuracy = 0.1)),\n",
    "            position = position_stack(vjust = 0.5),\n",
    "            hjust = -0.01,\n",
    "            color = \"black\", size = 3.2) +\n",
    "  labs(title = \"Percentage of Observations by Contract Type and Customer Default\",\n",
    "       x = NULL,  # Remove x-axis label\n",
    "       y = NULL) +\n",
    "  scale_x_continuous(labels = NULL, expand = expansion(c(0, 0.05))) +  # Remove x-axis values\n",
    "  scale_fill_manual(values = c(\"khaki\", \"darkseagreen\"), name = \"Customer Default\") +\n",
    "  theme_minimal() +\n",
    "  theme(plot.title = element_text(size = 10))+\n",
    "  theme(axis.text.y = element_text(size = 10),\n",
    "        axis.title.x = element_blank(),  # Remove x-axis title\n",
    "        legend.position = \"right\",\n",
    "        legend.direction = \"vertical\",\n",
    "        panel.grid.major = element_blank(),\n",
    "        panel.grid.minor = element_blank())\n",
    "```\n",
    "\n",
    "#### Income type\n",
    "\n",
    "Among the income type groups, those on maternity leave and unemployed\n",
    "have the highest default rates, with more than 35% of loans in default\n",
    "for these groups. The groups of students and business people do not have\n",
    "any individuals with defaults.\n",
    "\n",
    "``` {r}\n",
    "# Summarize data by income type and default\n",
    "data_summary <- app_train_clean %>%\n",
    "  group_by(NAME_INCOME_TYPE, TARGET) %>%\n",
    "  summarize(count = n(), .groups = 'drop') %>%\n",
    "  group_by(NAME_INCOME_TYPE) %>%\n",
    "  mutate(total = sum(count),  # Total count per income type\n",
    "         percentage = (count / total) * 100) %>%  # Percentage per income type\n",
    "  ungroup()\n",
    "\n",
    "# Extract percentage for TARGET == 0\n",
    "percentage_target_0 <- data_summary %>%\n",
    "  filter(TARGET == 0) %>%\n",
    "  select(NAME_INCOME_TYPE, percentage_target_0 = percentage)\n",
    "\n",
    "# Order and adjust data for plotting\n",
    "data_summary_ordered <- data_summary %>%\n",
    "  left_join(percentage_target_0, by = \"NAME_INCOME_TYPE\") %>%\n",
    "  mutate(order = if_else(is.na(percentage_target_0), 0, percentage_target_0)) %>%\n",
    "  arrange(desc(order)) %>%\n",
    "  mutate(NAME_INCOME_TYPE = factor(NAME_INCOME_TYPE, levels = unique(NAME_INCOME_TYPE)))\n",
    "\n",
    "# Income by default\n",
    "ggplot(data_summary_ordered, aes(x = percentage, y = reorder(NAME_INCOME_TYPE, -order), fill = as.factor(TARGET))) +\n",
    "  geom_bar(stat = \"identity\", position = \"stack\", alpha = 0.5) +  # Add transparency\n",
    "  geom_text(aes(label = scales::percent(percentage / 100, accuracy = 0.1)),\n",
    "            position = position_stack(vjust = 0.5),\n",
    "            hjust = -0.01,\n",
    "            color = \"black\", size = 3.2) +\n",
    "  labs(title = \"Percentage of Observations by Income Type and Customer Default\",\n",
    "       x = NULL,  # Remove x-axis label\n",
    "       y = NULL) +\n",
    "  scale_x_continuous(labels = NULL, expand = expansion(c(0, 0.05))) +  # Remove x-axis values\n",
    "  scale_fill_manual(values = c(\"khaki\", \"darkseagreen\"), name = \"Customer Default\") +\n",
    "  theme_minimal() +\n",
    "  theme(plot.title = element_text(size = 10))+\n",
    "  theme(axis.text.y = element_text(size = 10),\n",
    "        axis.title.x = element_blank(),  # Remove x-axis title\n",
    "        legend.position = \"right\",\n",
    "        legend.direction = \"vertical\",\n",
    "        panel.grid.major = element_blank(),\n",
    "        panel.grid.minor = element_blank())\n",
    "```\n",
    "\n",
    "#### Education\n",
    "\n",
    "Among the educational levels, it is notable that individuals with an\n",
    "academic degree who are in default have higher loan amounts compared to\n",
    "those who are not in default. This pattern is not significantly observed\n",
    "in other educational levels.\n",
    "\n",
    "``` {r}\n",
    "# Loan amount by education and default\n",
    "ggplot(app_train_clean, aes(x = as.factor(NAME_EDUCATION_TYPE), y = AMT_CREDIT, fill = as.factor(TARGET))) +\n",
    "  geom_boxplot() +\n",
    "  labs(title = \"Distribution of the Loan Amount by Education Type and Customer Default\", \n",
    "       x = \"Education Type\", \n",
    "       y = \"Amount of the Loan\") +\n",
    "  scale_y_continuous(labels = scales::label_number(scale = 1)) +\n",
    "  scale_fill_manual(values = c(\"khaki\", \"darkseagreen\"), name = \"Customer Default\") +\n",
    "  theme_minimal() +\n",
    "  theme(plot.title = element_text(size = 10)) +\n",
    "  theme(axis.text.x = element_text(hjust = 0.5)) +  # Center align the x-axis labels\n",
    "  scale_x_discrete(labels = function(x) gsub(\" \", \"\\n\", x))  # Replace spaces with newline characters in x-axis labels\n",
    "```\n",
    "\n",
    "Regarding education, all levels have default cases. However, the default\n",
    "rate for the “Lower Secondary” education group is six times higher\n",
    "compared to the “Academic Degree” group.\n",
    "\n",
    "``` {r}\n",
    "# Summarize data by education and default\n",
    "data_summary <- app_train_clean %>%\n",
    "  group_by(NAME_EDUCATION_TYPE, TARGET) %>%\n",
    "  summarize(count = n(), .groups = 'drop') %>%\n",
    "  group_by(NAME_EDUCATION_TYPE) %>%\n",
    "  mutate(total = sum(count),\n",
    "         percentage = (count / total) * 100) %>%\n",
    "  ungroup()\n",
    "\n",
    "# Extract percentage for TARGET == 0g\n",
    "percentage_target_0 <- data_summary %>%\n",
    "  filter(TARGET == 0) %>%\n",
    "  select(NAME_EDUCATION_TYPE, percentage_target_0 = percentage)\n",
    "\n",
    "# Order and adjust data for plotting\n",
    "data_summary_ordered <- data_summary %>%\n",
    "  left_join(percentage_target_0, by = \"NAME_EDUCATION_TYPE\") %>%\n",
    "  mutate(order = if_else(is.na(percentage_target_0), 0, percentage_target_0)) %>%\n",
    "  arrange(desc(order)) %>%\n",
    "  mutate(NAME_EDUCATION_TYPE = factor(NAME_EDUCATION_TYPE, levels = unique(NAME_EDUCATION_TYPE)))\n",
    "\n",
    "# Education by default\n",
    "ggplot(data_summary_ordered, aes(x = percentage, y = reorder(NAME_EDUCATION_TYPE, -order), fill = as.factor(TARGET))) +\n",
    "  geom_bar(stat = \"identity\", position = \"stack\", alpha = 0.5) +  # Add transparency\n",
    "  geom_text(aes(label = scales::percent(percentage / 100, accuracy = 0.1)),\n",
    "            position = position_stack(vjust = 0.5),\n",
    "            hjust = -0.01,\n",
    "            color = \"black\", size = 3.2) +\n",
    "  labs(title = \"Percentage of Observations by Education level \\nand Customer Default\",\n",
    "       x = NULL,  # Remove x-axis label\n",
    "       y = NULL) +\n",
    "  scale_x_continuous(labels = NULL, expand = expansion(c(0, 0.05))) +  # Remove x-axis values\n",
    "  scale_fill_manual(values = c(\"khaki\", \"darkseagreen\"), name = \"Customer Default\") +\n",
    "  theme_minimal() +\n",
    "  theme(plot.title = element_text(size = 10))+\n",
    "  theme(axis.text.y = element_text(size = 10),\n",
    "        axis.title.x = element_blank(),  # Remove x-axis title\n",
    "        legend.position = \"right\",\n",
    "        legend.direction = \"vertical\",\n",
    "        panel.grid.major = element_blank(),\n",
    "        panel.grid.minor = element_blank())\n",
    "```\n",
    "\n",
    "#### Family Status\n",
    "\n",
    "The default rates by family status are quite similar, with a notable\n",
    "exception for the lower percentage of defaults among widows. There are\n",
    "no defaults reported among those with unknown family status.\n",
    "\n",
    "``` {r}\n",
    "# Summarize data by family and default\n",
    "data_summary <- app_train_clean %>%\n",
    "  group_by(NAME_FAMILY_STATUS, TARGET) %>%\n",
    "  summarize(count = n(), .groups = 'drop') %>%\n",
    "  group_by(NAME_FAMILY_STATUS) %>%\n",
    "  mutate(total = sum(count),\n",
    "         percentage = (count / total) * 100) %>%\n",
    "  ungroup()\n",
    "\n",
    "# Extract percentage for TARGET == 0\n",
    "percentage_target_0 <- data_summary %>%\n",
    "  filter(TARGET == 0) %>%\n",
    "  select(NAME_FAMILY_STATUS, percentage_target_0 = percentage)\n",
    "\n",
    "# Order and adjust data for plotting\n",
    "data_summary_ordered <- data_summary %>%\n",
    "  left_join(percentage_target_0, by = \"NAME_FAMILY_STATUS\") %>%\n",
    "  mutate(order = if_else(is.na(percentage_target_0), 0, percentage_target_0)) %>%\n",
    "  arrange(desc(order)) %>%\n",
    "  mutate(NAME_FAMILY_STATUS = factor(NAME_FAMILY_STATUS, levels = unique(NAME_FAMILY_STATUS)))\n",
    "\n",
    "# Family status by default\n",
    "ggplot(data_summary_ordered, aes(x = percentage, y = reorder(NAME_FAMILY_STATUS, -order), fill = as.factor(TARGET))) +\n",
    "  geom_bar(stat = \"identity\", position = \"stack\", alpha = 0.5) +  # Add transparency\n",
    "  geom_text(aes(label = scales::percent(percentage / 100, accuracy = 0.1)),\n",
    "            position = position_stack(vjust = 0.5),\n",
    "            hjust = -0.01,\n",
    "            color = \"black\", size = 3.2) +\n",
    "  labs(title = \"Percentage of Observations by Family Status and Customer Default\",\n",
    "       x = NULL,  # Remove x-axis label\n",
    "       y = NULL) +\n",
    "  scale_x_continuous(labels = NULL, expand = expansion(c(0, 0.05))) +  # Remove x-axis values\n",
    "  scale_fill_manual(values = c(\"khaki\", \"darkseagreen\"), name = \"Customer Default\") +\n",
    "  theme_minimal() +\n",
    "  theme(plot.title = element_text(size = 10))+\n",
    "  theme(axis.text.y = element_text(size = 10),\n",
    "        axis.title.x = element_blank(),  # Remove x-axis title\n",
    "        legend.position = \"right\",\n",
    "        legend.direction = \"vertical\",\n",
    "        panel.grid.major = element_blank(),\n",
    "        panel.grid.minor = element_blank())\n",
    "```\n",
    "\n",
    "#### Housing type\n",
    "\n",
    "Consumers with different housing types exhibit varying default rates.\n",
    "Notably, those living in rented apartments or with their parents have\n",
    "default rates close to 12%. The lowest default rate is observed among\n",
    "those living in office apartments, at 6.6%.\n",
    "\n",
    "``` {r}\n",
    "# Summarize data by housing type and default\n",
    "data_summary <- app_train_clean %>%\n",
    "  group_by(NAME_HOUSING_TYPE, TARGET) %>%\n",
    "  summarize(count = n(), .groups = 'drop') %>%\n",
    "  group_by(NAME_HOUSING_TYPE) %>%\n",
    "  mutate(total = sum(count),\n",
    "         percentage = (count / total) * 100) %>%\n",
    "  ungroup()\n",
    "\n",
    "# Extract percentage for TARGET == 0\n",
    "percentage_target_0 <- data_summary %>%\n",
    "  filter(TARGET == 0) %>%\n",
    "  select(NAME_HOUSING_TYPE, percentage_target_0 = percentage)\n",
    "\n",
    "# Order and adjust data for plotting\n",
    "data_summary_ordered <- data_summary %>%\n",
    "  left_join(percentage_target_0, by = \"NAME_HOUSING_TYPE\") %>%\n",
    "  mutate(order = if_else(is.na(percentage_target_0), 0, percentage_target_0)) %>%\n",
    "  arrange(desc(order)) %>%\n",
    "  mutate(NAME_HOUSING_TYPE = factor(NAME_HOUSING_TYPE, levels = unique(NAME_HOUSING_TYPE)))\n",
    "\n",
    "# Housing by default\n",
    "ggplot(data_summary_ordered, aes(x = percentage, y = reorder(NAME_HOUSING_TYPE, -order), fill = as.factor(TARGET))) +\n",
    "  geom_bar(stat = \"identity\", position = \"stack\", alpha = 0.5) +  # Add transparency\n",
    "  geom_text(aes(label = scales::percent(percentage / 100, accuracy = 0.1)),\n",
    "            position = position_stack(vjust = 0.5),\n",
    "            hjust = -0.01,\n",
    "            color = \"black\", size = 3.2) +\n",
    "  labs(title = \"Percentage of Observations by Housing Type and Customer Default\",\n",
    "       x = NULL,  # Remove x-axis label\n",
    "       y = NULL) +\n",
    "  scale_x_continuous(labels = NULL, expand = expansion(c(0, 0.05))) +  # Remove x-axis values\n",
    "  scale_fill_manual(values = c(\"khaki\", \"darkseagreen\"), name = \"Customer Default\") +\n",
    "  theme_minimal() +\n",
    "  theme(plot.title = element_text(size = 10))+\n",
    "  theme(axis.text.y = element_text(size = 10),\n",
    "        axis.title.x = element_blank(),  # Remove x-axis title\n",
    "        legend.position = \"right\",\n",
    "        legend.direction = \"vertical\",\n",
    "        panel.grid.major = element_blank(),\n",
    "        panel.grid.minor = element_blank())\n",
    "```\n",
    "\n",
    "#### Occupation type\n",
    "\n",
    "Among the occupation types, 96,391 observations were blank and have been\n",
    "transformed to “Unknown”.\n",
    "\n",
    "\\`\\`\\`{r transformation 5} \\# Convert OCCUPATION_TYPE to factor if not\n",
    "already done\n",
    "app_train_clean$OCCUPATION_TYPE <- as.factor(app_train_clean$OCCUPATION_TYPE)\n",
    "\n",
    "# Replace empty strings with “Unknown”\n",
    "\n",
    "app_train_clean$OCCUPATION_TYPE <- as.character(app_train_clean$OCCUPATION_TYPE)\n",
    "\\# Convert to character\n",
    "app_train_clean$OCCUPATION_TYPE[app_train_clean$OCCUPATION_TYPE == ““\\]\n",
    "\\<-”Unknown” \\# Replace empty strings\n",
    "app_train_clean$OCCUPATION_TYPE <- as.factor(app_train_clean$OCCUPATION_TYPE)\n",
    "\\# Convert back to factor\n",
    "\n",
    "\n",
    "    Among the occupation types, low-skill laborers have the highest default rate at 17.2%.\n",
    "\n",
    "    ```{r}\n",
    "    # Summarize data by occupation type and default\n",
    "    data_summary <- app_train_clean %>%\n",
    "      group_by(OCCUPATION_TYPE, TARGET) %>%\n",
    "      summarize(count = n(), .groups = 'drop') %>%\n",
    "      group_by(OCCUPATION_TYPE) %>%\n",
    "      mutate(total = sum(count),\n",
    "             percentage = (count / total) * 100) %>%\n",
    "      ungroup()\n",
    "\n",
    "    # Extract percentage for TARGET == 0\n",
    "    percentage_target_0 <- data_summary %>%\n",
    "      filter(TARGET == 0) %>%\n",
    "      select(OCCUPATION_TYPE, percentage_target_0 = percentage)\n",
    "\n",
    "    # Order and adjust data for plotting\n",
    "    data_summary_ordered <- data_summary %>%\n",
    "      left_join(percentage_target_0, by = \"OCCUPATION_TYPE\") %>%\n",
    "      mutate(order = if_else(is.na(percentage_target_0), 0, percentage_target_0)) %>%\n",
    "      arrange(desc(order)) %>%\n",
    "      mutate(OCCUPATION_TYPE = factor(OCCUPATION_TYPE, levels = unique(OCCUPATION_TYPE)))\n",
    "\n",
    "    # Occupation by default\n",
    "    ggplot(data_summary_ordered, aes(x = percentage, y = reorder(OCCUPATION_TYPE, -order), fill = as.factor(TARGET))) +\n",
    "      geom_bar(stat = \"identity\", position = \"stack\", alpha = 0.5) +  # Add transparency\n",
    "      geom_text(aes(label = scales::percent(percentage / 100, accuracy = 0.1)),\n",
    "                position = position_stack(vjust = 0.5),\n",
    "                hjust = -0.01,\n",
    "                color = \"black\", size = 3.2) +\n",
    "      labs(title = \"Percentage of Observations by Occupation Type and Customer Default\",\n",
    "           x = NULL,  # Remove x-axis label\n",
    "           y = NULL) +\n",
    "      scale_x_continuous(labels = NULL, expand = expansion(c(0, 0.05))) +  # Remove x-axis values\n",
    "      scale_fill_manual(values = c(\"khaki\", \"darkseagreen\"), name = \"Customer Default\") +\n",
    "      theme_minimal() +\n",
    "      theme(plot.title = element_text(size = 10))+\n",
    "      theme(axis.text.y = element_text(size = 10),\n",
    "            axis.title.x = element_blank(),  # Remove x-axis title\n",
    "            legend.position = \"right\",\n",
    "            legend.direction = \"vertical\",\n",
    "            panel.grid.major = element_blank(),\n",
    "            panel.grid.minor = element_blank())\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "#### 3.3 Additional Observations\n",
    "\n",
    "#### Loan amount and default\n",
    "\n",
    "The average loan amounts are similar between the groups that default\n",
    "(\\$557,778) and those that do not (\\$602,648).\n",
    "\n",
    "``` {r}\n",
    "# Distribution of loan amount by default\n",
    "ggplot(app_train_clean, aes(x = as.factor(TARGET), y = AMT_CREDIT, fill = as.factor(TARGET))) +\n",
    "  geom_boxplot() +\n",
    "  labs(title = \"Distribution of the loan amount by customer default.\", \n",
    "       x = \"customer default\", \n",
    "       y = \"amount of the loan\") +\n",
    "  scale_y_continuous(labels = scales::label_number(scale = 1)) +\n",
    "  scale_fill_manual(values = c(\"khaki\", \"darkseagreen\")) +\n",
    "  theme_minimal() +\n",
    "  theme(plot.title = element_text(size = 10))+\n",
    "  theme(legend.position = \"none\")\n",
    "```\n",
    "\n",
    "The distribution of borrowers in relation to the loan amounts is\n",
    "left-skewed, with the same pattern of skewness at different intensities\n",
    "across the target variable groups.\n",
    "\n",
    "``` {r}\n",
    "# Histogram - distribution of loan amount by default\n",
    "ggplot(app_train_clean, aes(x = AMT_CREDIT, fill = as.factor(TARGET))) +\n",
    "  geom_histogram(alpha = 0.6, bins = 30, position = \"identity\") +\n",
    "  labs(title = \"Distribution of the loan amount by customer default.\", \n",
    "       x = \"amount of the loan\", \n",
    "       y = \"Frequency\") +\n",
    "  scale_x_continuous(labels = scales::label_number(scale = 1)) +\n",
    "  scale_fill_manual(values = c(\"khaki\", \"darkseagreen\"), name = \"customer default\") +\n",
    "  theme_minimal() +\n",
    "  theme(plot.title = element_text(size = 10))+\n",
    "  theme(legend.position = \"right\")\n",
    "```\n",
    "\n",
    "#### Loan amount, income and default\n",
    "\n",
    "The graph below aims to explore the variation in loan amounts across\n",
    "income groups and default status. The income groups were defined based\n",
    "on the distribution of the majority of the data set.\n",
    "\n",
    "``` {r}\n",
    "# Income levels\n",
    "income_b <- app_train_clean %>%\n",
    "  mutate(income_bracket = cut(AMT_INCOME_TOTAL, \n",
    "                              breaks = c(seq(0, 2000000, by = 500000), Inf), \n",
    "                              labels = c(\n",
    "                                \"$0 - $500k\", \n",
    "                                \"$500k - $1M\", \n",
    "                                \"$1M - $1,5M\", \n",
    "                                \"$1,5M - $2M\", \n",
    "                                \"Above $2M\")))\n",
    "\n",
    "# Plot for loan by income and default\n",
    "ggplot(income_b, aes(x = income_bracket, y = AMT_CREDIT, fill = as.factor(TARGET))) +\n",
    "  geom_boxplot() +\n",
    "  labs(title = \"Loan Amount by Income Bracket and Customer Default\",\n",
    "       x = \"Income Bracket\",\n",
    "       y = \"Loan Amount\") +\n",
    "  scale_fill_manual(values = c(\"khaki\", \"darkseagreen\"), name = \"Customer Default\") +\n",
    "  scale_y_continuous(labels = scales::label_number(big.mark = \",\")) +\n",
    "  theme_minimal() +\n",
    "  theme(plot.title = element_text(size = 10))+\n",
    "  theme(axis.text.x = element_text(),\n",
    "        axis.text.y = element_text(),\n",
    "        legend.position = \"right\",\n",
    "        panel.grid.major.x = element_blank(),  \n",
    "        panel.grid.minor.x = element_blank(),  \n",
    "        panel.grid.major.y = element_line(colour = \"grey80\"), \n",
    "        panel.grid.minor.y = element_blank())  \n",
    "```\n",
    "\n",
    "Among consumers with incomes above \\$500,000, the average loan amounts\n",
    "tend to be lower for those who default compared to those who pay on\n",
    "time. The exception is the income range between \\$1 million and \\$1.5\n",
    "million, where the average loan amounts are very similar for both\n",
    "groups.\n",
    "\n",
    "#### External sources\n",
    "\n",
    "Credit scores generally serve as a good indicator of whether consumers\n",
    "are likely to delay payments or not. Among the variables used by Home\n",
    "Credit, there are three sources providing such information. These scores\n",
    "are normalized to be comparable, with values ranging from 0 to 1, where\n",
    "lower scores indicate higher risk of default and higher scores represent\n",
    "lower risk of default.\n",
    "\n",
    "``` {r}\n",
    "# Function to calculate count and percentage of non-missing and missing values for each EXT_SOURCE\n",
    "calculate_percentages <- function(data, source) {\n",
    "  data %>%\n",
    "    summarise(\n",
    "      Filled = sum(!is.na(.data[[source]])),\n",
    "      Missing = sum(is.na(.data[[source]])),\n",
    "      Total = n()\n",
    "    ) %>%\n",
    "    mutate(\n",
    "      Percent_Filled = (Filled / Total) * 100,\n",
    "      Percent_Missing = (Missing / Total) * 100,\n",
    "      Source = source\n",
    "    ) %>%\n",
    "    pivot_longer(cols = c(Filled, Missing), names_to = \"Status\", values_to = \"Count\") %>%\n",
    "    pivot_longer(cols = c(Percent_Filled, Percent_Missing), names_to = \"Percentage_Status\", values_to = \"Percentage\") %>%\n",
    "    separate(Percentage_Status, into = c(\"Type\", \"Status\"), sep = \"_\") %>%\n",
    "    filter(Type == \"Percent\")\n",
    "}\n",
    "\n",
    "# Apply the function to each EXT_SOURCE\n",
    "ext_source_1 <- calculate_percentages(app_train_clean, \"EXT_SOURCE_1\")\n",
    "ext_source_2 <- calculate_percentages(app_train_clean, \"EXT_SOURCE_2\")\n",
    "ext_source_3 <- calculate_percentages(app_train_clean, \"EXT_SOURCE_3\")\n",
    "\n",
    "# Combining the sources\n",
    "combined_data <- bind_rows(ext_source_1, ext_source_2, ext_source_3)\n",
    "\n",
    "# Plot of external source percentages\n",
    "ggplot(combined_data, aes(x = Source, y = Percentage, fill = Status)) +\n",
    "  geom_col(position = \"dodge\", alpha = 0.4) + \n",
    "  geom_text(aes(label = round(Percentage, 1)), position = position_dodge(width = 0.9), vjust = -0.5, size = 2.5) +\n",
    "  labs(title = \"Percentage of Filled and Missing Values for External Source Variables\", \n",
    "       x = \"Credit Score Sources\",\n",
    "       y = \"Percentage (%)\") +\n",
    "  scale_fill_manual(values = c(\"lightblue\", \"firebrick\"), labels = c(\"Filled\", \"Missing\")) +\n",
    "  theme_minimal() +\n",
    "  theme(axis.text.x = element_text(angle = 0, hjust = 0.5),\n",
    "        plot.title = element_text(size = 11))\n",
    "```\n",
    "\n",
    "For almost all individuals observed, there is information available from\n",
    "SOURCE 2. For SOURCE 3, more than 70% of individuals have a score, while\n",
    "for SOURCE 1, less than 50% have available data.\n",
    "\n",
    "**Density plots**\n",
    "\n",
    "The graphs below illustrate the density distributions between consumers\n",
    "who are in default and those who are not.\n",
    "\n",
    "**Source 1**\n",
    "\n",
    "``` {r}\n",
    "# Density of ext_source_1 scores by default\n",
    "ggplot(app_train_clean, aes(x = EXT_SOURCE_1, color = as.factor(TARGET), fill = as.factor(TARGET))) +\n",
    "  stat_density(geom = \"line\", position = \"identity\") +\n",
    "  stat_density(geom = \"area\", position = \"identity\", alpha = 0.3) +\n",
    "  labs(title = \"Density of EXT_SOURCE_1 by Customer Default\", \n",
    "       x = \"EXT_SOURCE_1\", \n",
    "       y = \"Density\") +\n",
    "  scale_color_manual(values = c(\"khaki\", \"darkseagreen\"), name = \"Customer Default\") +\n",
    "  scale_fill_manual(values = c(\"khaki\", \"darkseagreen\"), name = \"Customer Default\") +\n",
    "  theme_minimal()+\n",
    "  theme(plot.title = element_text(size = 10))\n",
    "```\n",
    "\n",
    "**Source 2**\n",
    "\n",
    "``` {r}\n",
    "# Density of ext_source_2 scores by default\n",
    "ggplot(app_train_clean, aes(x = EXT_SOURCE_2, color = as.factor(TARGET), fill = as.factor(TARGET))) +\n",
    "  stat_density(geom = \"line\", position = \"identity\") +\n",
    "  stat_density(geom = \"area\", position = \"identity\", alpha = 0.3) +\n",
    "  labs(title = \"Density of EXT_SOURCE_1 by Customer Default\", \n",
    "       x = \"EXT_SOURCE_2\", \n",
    "       y = \"Density\") +\n",
    "  scale_color_manual(values = c(\"khaki\", \"darkseagreen\"), name = \"Customer Default\") +\n",
    "  scale_fill_manual(values = c(\"khaki\", \"darkseagreen\"), name = \"Customer Default\") +\n",
    "  theme_minimal()+\n",
    "  theme(plot.title = element_text(size = 10))\n",
    "```\n",
    "\n",
    "**Source 3**\n",
    "\n",
    "``` {r}\n",
    "# Density of ext_source_3 scores by default\n",
    "ggplot(app_train_clean, aes(x = EXT_SOURCE_3, color = as.factor(TARGET), fill = as.factor(TARGET))) +\n",
    "  stat_density(geom = \"line\", position = \"identity\") +\n",
    "  stat_density(geom = \"area\", position = \"identity\", alpha = 0.3) +\n",
    "  labs(title = \"Density of EXT_SOURCE_1 by Customer Default\", \n",
    "       x = \"EXT_SOURCE_3\", \n",
    "       y = \"Density\") +\n",
    "  scale_color_manual(values = c(\"khaki\", \"darkseagreen\"), name = \"Customer Default\") +\n",
    "  scale_fill_manual(values = c(\"khaki\", \"darkseagreen\"), name = \"Customer Default\") +\n",
    "  theme_minimal() +\n",
    "  theme(plot.title = element_text(size = 10))\n",
    "```\n",
    "\n",
    "Below is the numeric distribution of scores by default status for each\n",
    "source.\n",
    "\n",
    "``` {r}\n",
    "# Select variables EXT_SOURCE_1, EXT_SOURCE_2, e EXT_SOURCE_3\n",
    "app_train_filtered <- app_train_clean %>%\n",
    "  select(TARGET, EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3) %>%\n",
    "  pivot_longer(cols = starts_with(\"EXT_SOURCE\"), \n",
    "               names_to = \"Source\", \n",
    "               values_to = \"Score\") %>%\n",
    "  mutate(Source = factor(Source, levels = c(\"EXT_SOURCE_1\", \"EXT_SOURCE_2\", \"EXT_SOURCE_3\")))\n",
    "\n",
    "# Plot the three variables together\n",
    "ggplot(app_train_filtered, aes(x = as.factor(TARGET), y = Score, fill = as.factor(TARGET))) +\n",
    "  geom_boxplot() +\n",
    "  labs(title = \"Distribution of EXT_SOURCE Scores by Customer Default\",\n",
    "       x = \"Customer Default\",\n",
    "       y = \"Score\") +\n",
    "  scale_y_continuous(labels = scales::label_number(scale = 1)) +\n",
    "  scale_fill_manual(values = c(\"khaki\", \"darkseagreen\")) +\n",
    "  theme_minimal() +\n",
    "  theme(plot.title = element_text(size = 10))+\n",
    "  theme(legend.position = \"none\",\n",
    "        axis.text.x = element_text(size = 8),\n",
    "        axis.text.y = element_text(size = 8)) +\n",
    "  facet_wrap(~Source, scales = \"free_y\") \n",
    "```\n",
    "\n",
    "In the box plot above, comparing the three sources, it is clear that, on\n",
    "average, lower scores are observed among consumers who default. However,\n",
    "there are individuals with low scores who make their payments regularly,\n",
    "as well as those with high scores who end up defaulting.\n",
    "\n",
    "The chart below clearly shows the percentages of defaults based on the\n",
    "scores.\n",
    "\n",
    "``` {r}\n",
    "# Filter dataset to include only rows where EXT_SOURCE variables are not missing\n",
    "app_train_clean_filtered <- app_train_clean %>%\n",
    "  filter(!is.na(EXT_SOURCE_1) | !is.na(EXT_SOURCE_2) | !is.na(EXT_SOURCE_3))\n",
    "\n",
    "# Define intervals for EXT_SOURCE variables with \"> 0.9\" at the end\n",
    "app_train_clean_filtered <- app_train_clean_filtered %>%\n",
    "  mutate(score_levels_1 = case_when(\n",
    "    EXT_SOURCE_1 <= 0.1 ~ \"0 - 0.1\",\n",
    "    EXT_SOURCE_1 <= 0.2 ~ \"0.1 - 0.2\",\n",
    "    EXT_SOURCE_1 <= 0.3 ~ \"0.2 - 0.3\",\n",
    "    EXT_SOURCE_1 <= 0.4 ~ \"0.3 - 0.4\",\n",
    "    EXT_SOURCE_1 <= 0.5 ~ \"0.4 - 0.5\",\n",
    "    EXT_SOURCE_1 <= 0.6 ~ \"0.5 - 0.6\",\n",
    "    EXT_SOURCE_1 <= 0.7 ~ \"0.6 - 0.7\",\n",
    "    EXT_SOURCE_1 <= 0.8 ~ \"0.7 - 0.8\",\n",
    "    EXT_SOURCE_1 <= 0.9 ~ \"0.8 - 0.9\",\n",
    "    TRUE ~ \"> 0.9\"\n",
    "  )) %>%\n",
    "  mutate(score_levels_2 = case_when(\n",
    "    EXT_SOURCE_2 <= 0.1 ~ \"0 - 0.1\",\n",
    "    EXT_SOURCE_2 <= 0.2 ~ \"0.1 - 0.2\",\n",
    "    EXT_SOURCE_2 <= 0.3 ~ \"0.2 - 0.3\",\n",
    "    EXT_SOURCE_2 <= 0.4 ~ \"0.3 - 0.4\",\n",
    "    EXT_SOURCE_2 <= 0.5 ~ \"0.4 - 0.5\",\n",
    "    EXT_SOURCE_2 <= 0.6 ~ \"0.5 - 0.6\",\n",
    "    EXT_SOURCE_2 <= 0.7 ~ \"0.6 - 0.7\",\n",
    "    EXT_SOURCE_2 <= 0.8 ~ \"0.7 - 0.8\",\n",
    "    EXT_SOURCE_2 <= 0.9 ~ \"0.8 - 0.9\",\n",
    "    TRUE ~ \"> 0.9\"\n",
    "  )) %>%\n",
    "  mutate(score_levels_3 = case_when(\n",
    "    EXT_SOURCE_3 <= 0.1 ~ \"0 - 0.1\",\n",
    "    EXT_SOURCE_3 <= 0.2 ~ \"0.1 - 0.2\",\n",
    "    EXT_SOURCE_3 <= 0.3 ~ \"0.2 - 0.3\",\n",
    "    EXT_SOURCE_3 <= 0.4 ~ \"0.3 - 0.4\",\n",
    "    EXT_SOURCE_3 <= 0.5 ~ \"0.4 - 0.5\",\n",
    "    EXT_SOURCE_3 <= 0.6 ~ \"0.5 - 0.6\",\n",
    "    EXT_SOURCE_3 <= 0.7 ~ \"0.6 - 0.7\",\n",
    "    EXT_SOURCE_3 <= 0.8 ~ \"0.7 - 0.8\",\n",
    "    EXT_SOURCE_3 <= 0.9 ~ \"0.8 - 0.9\",\n",
    "    TRUE ~ \"> 0.9\"\n",
    "  ))\n",
    "\n",
    "# Combine the data into a long format for faceting\n",
    "counts_df <- bind_rows(\n",
    "  app_train_clean_filtered %>%\n",
    "    filter(!is.na(EXT_SOURCE_1)) %>%\n",
    "    group_by(score_levels_1, TARGET) %>%\n",
    "    summarise(n = n(), .groups = 'drop') %>%\n",
    "    pivot_wider(names_from = TARGET, values_from = n, names_prefix = \"n_TARGET_\") %>%\n",
    "    mutate(percent_default = round((n_TARGET_1 / (n_TARGET_1 + n_TARGET_0)) * 100, 1)) %>%\n",
    "    rename(`Default_no` = `n_TARGET_0`, `Default_yes` = `n_TARGET_1`) %>%\n",
    "    mutate(Source = \"EXT_SOURCE_1\", score_levels = factor(score_levels_1, levels = c(\n",
    "      \"0 - 0.1\", \"0.1 - 0.2\", \"0.2 - 0.3\", \"0.3 - 0.4\", \"0.4 - 0.5\",\n",
    "      \"0.5 - 0.6\", \"0.6 - 0.7\", \"0.7 - 0.8\", \"0.8 - 0.9\", \"> 0.9\"\n",
    "    ))),\n",
    "  \n",
    "  app_train_clean_filtered %>%\n",
    "    filter(!is.na(EXT_SOURCE_2)) %>%\n",
    "    group_by(score_levels_2, TARGET) %>%\n",
    "    summarise(n = n(), .groups = 'drop') %>%\n",
    "    pivot_wider(names_from = TARGET, values_from = n, names_prefix = \"n_TARGET_\") %>%\n",
    "    mutate(percent_default = round((n_TARGET_1 / (n_TARGET_1 + n_TARGET_0)) * 100, 1)) %>%\n",
    "    rename(`Default_no` = `n_TARGET_0`, `Default_yes` = `n_TARGET_1`) %>%\n",
    "    mutate(Source = \"EXT_SOURCE_2\", score_levels = factor(score_levels_2, levels = c(\n",
    "      \"0 - 0.1\", \"0.1 - 0.2\", \"0.2 - 0.3\", \"0.3 - 0.4\", \"0.4 - 0.5\",\n",
    "      \"0.5 - 0.6\", \"0.6 - 0.7\", \"0.7 - 0.8\", \"0.8 - 0.9\", \"> 0.9\"\n",
    "    ))),\n",
    "  \n",
    "  app_train_clean_filtered %>%\n",
    "    filter(!is.na(EXT_SOURCE_3)) %>%\n",
    "    group_by(score_levels_3, TARGET) %>%\n",
    "    summarise(n = n(), .groups = 'drop') %>%\n",
    "    pivot_wider(names_from = TARGET, values_from = n, names_prefix = \"n_TARGET_\") %>%\n",
    "    mutate(percent_default = round((n_TARGET_1 / (n_TARGET_1 + n_TARGET_0)) * 100, 1)) %>%\n",
    "    rename(`Default_no` = `n_TARGET_0`, `Default_yes` = `n_TARGET_1`) %>%\n",
    "    mutate(Source = \"EXT_SOURCE_3\", score_levels = factor(score_levels_3, levels = c(\n",
    "      \"0 - 0.1\", \"0.1 - 0.2\", \"0.2 - 0.3\", \"0.3 - 0.4\", \"0.4 - 0.5\",\n",
    "      \"0.5 - 0.6\", \"0.6 - 0.7\", \"0.7 - 0.8\", \"0.8 - 0.9\", \"> 0.9\"\n",
    "    )))\n",
    ") %>%\n",
    "  select(Source, score_levels, percent_default)\n",
    "\n",
    "# Create the facet wrap plot with gradient colors and no legend\n",
    "ggplot(counts_df, aes(x = score_levels, y = percent_default, fill = percent_default)) +\n",
    "  geom_bar(stat = \"identity\", alpha = 0.7) +\n",
    "  geom_text(aes(label = round(percent_default, 1)), position = position_dodge(width = 0.9), vjust = -0.5, size = 2.5) +\n",
    "  labs(title = \"Percentage of Default by EXT_SOURCE Score Intervals\",\n",
    "       x = \"Score Intervals\",\n",
    "       y = \"Percentage of Default (%)\") +\n",
    "  scale_fill_gradient(low = \"lightblue\", high = \"firebrick\") +\n",
    "  theme_minimal() +\n",
    "  theme(plot.title = element_text(size = 10)) +\n",
    "  theme(\n",
    "    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),\n",
    "    axis.text.y = element_text(size = 8),\n",
    "    legend.position = \"none\"  # Remove the legend\n",
    "  ) +\n",
    "  facet_wrap(~Source, scales = \"free_x\")\n",
    "```\n",
    "\n",
    "As expected, lower scores tend to show a higher percentage of defaults.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "#### 3.4 Correlations\n",
    "\n",
    "The tables below display the main correlations between numerical\n",
    "variables and default.\n",
    "\n",
    "\\`\\`\\`{r, message=FALSE, warning=FALSE} \\# Create the correlation matrix\n",
    "cor_matrix \\<- cor(app_train_clean %\\>% select_if(is.numeric), use =\n",
    "“complete.obs”)\n",
    "\n",
    "# Extract the target correlations\n",
    "\n",
    "target_correlations \\<- cor_matrix\\[“TARGET”, \\]\n",
    "\n",
    "# Remove the correlation of TARGET with itself (correlation of 1)\n",
    "\n",
    "target_correlations \\<- target_correlations\\[names(target_correlations)\n",
    "!= “TARGET”\\]\n",
    "\n",
    "# Sort the correlations\n",
    "\n",
    "sorted_correlations \\<- sort(target_correlations, decreasing = TRUE)\n",
    "\n",
    "# Ten most positive correlations\n",
    "\n",
    "knitr::kable(head(sorted_correlations, 10), caption = “Ten most Positive\n",
    "correlations”)\n",
    "\n",
    "\n",
    "    In addition to the data exploration conducted so far, the table above shows the ten highest positive correlations between numerical variables and default.\n",
    "\n",
    "    It is noteworthy that variables related to consumers' social circles and the regions where they live are present.\n",
    "    The submission of document 3, the age of the car, and the time since a new ID issuance are also among the strongest correlations.\n",
    "\n",
    "    ```{r}\n",
    "    # Sort the correlations\n",
    "    sorted_correlations <- sort(target_correlations)\n",
    "\n",
    "    # Ten most negative correlations\n",
    "    knitr::kable(head(sorted_correlations, 10), caption = \"Ten most Negative correlations\")\n",
    "\n",
    "Among the negative correlations—where higher values are associated with\n",
    "a lower occurrence of default—external credit information stands out,\n",
    "with values approximately two to three times larger than the fourth\n",
    "variable. An increase in years of employment, age, and income also\n",
    "contributes to a lower likelihood of default. Additionally, variables\n",
    "related to individuals’ housing show that the larger these values are,\n",
    "the lower the probability of default.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "#### 3.5 Evaluation of Prediction Baselines\n",
    "\n",
    "**Naive baseline and majority classifier**\n",
    "\n",
    "The naive baseline and the Majority Rule Classifier both predict that no\n",
    "customer defaults, as 91.9% of customers do not default (TARGET=0). This\n",
    "results in an accuracy of 91.9% for the model; however, it fails to\n",
    "identify any defaulters, missing the 8.1% who actually do default. Thus,\n",
    "even though the accuracy appears high, this approach is not effective\n",
    "for identifying customers at risk of default. A more advanced model is\n",
    "needed to accurately classify both defaulters and non-defaulters.\n",
    "\n",
    "**Random classifier**\n",
    "\n",
    "The Random Classifier makes predictions randomly based on the\n",
    "distribution of classes in the data set. With 92% of the TARGET being 0\n",
    "and 8% being 1, the classifier will predict TARGET = 0 with a 92%\n",
    "probability and TARGET = 1 with an 8% probability.\n",
    "\n",
    "The expected accuracy reflects the class distribution, resulting in an\n",
    "expected accuracy of about 92% for TARGET = 0 and 8% for TARGET = 1,\n",
    "similar to the majority rule classifier.\n",
    "\n",
    "The simulation below is intended to illustrate the accuracy of using a\n",
    "random classifier for this data set.\n",
    "\n",
    "``` {r}\n",
    "# Seed for reproducibility\n",
    "set.seed(seed) \n",
    "\n",
    "# Number of predictions\n",
    "num_predictions <- nrow(app_train_clean)\n",
    "\n",
    "# Probabilities for each class\n",
    "prob_0 <- 0.92\n",
    "prob_1 <- 0.08\n",
    "\n",
    "# Generate random predictions based on the probabilities\n",
    "random_predictions <- sample(c(0, 1), size = num_predictions, replace = TRUE, prob = c(prob_0, prob_1))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_random <- round(mean(random_predictions == app_train_clean$TARGET),2)\n",
    "accuracy_random\n",
    "```\n",
    "\n",
    "Given that a Random Classifier achieves an accuracy of 85%, I expect the\n",
    "developed model to outperform this benchmark, showing improvements in\n",
    "both precision and recall, thereby enhancing its ability to accurately\n",
    "identify true positives and minimize false negatives.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### 4. Pre-Modeling\n",
    "\n",
    "#### 4.1 Dummy Encoding\n",
    "\n",
    "Before proceeding with the dummy encoding process, we will remove\n",
    "instances with missing CODE_Gender values. Additionally, the variable\n",
    "created to indicate individuals with an infinite number of employment\n",
    "days will be converted into a factor.\n",
    "\n",
    "In addition, all non-numeric variables will be selected to perform dummy\n",
    "encoding, ensuring they are properly transformed for analysis.\n",
    "\n",
    "\\`\\`\\`{r transformation 6} \\# Filter out instances with ‘XNA’ in\n",
    "CODE_GENDER and remove unused levels app_train_clean \\<-\n",
    "app_train_clean\\[app_train_clean$CODE_GENDER != \"XNA\", ] app_train_clean$CODE_GENDER\n",
    "\\<- droplevels(app_train_clean\\$CODE_GENDER)\n",
    "\n",
    "# Convert the DAYS_EMPLOYED_ANOM variable to a factor\n",
    "\n",
    "app_train_clean$DAYS_EMPLOYED_ANOM <- as.factor(app_train_clean$DAYS_EMPLOYED_ANOM)\n",
    "\n",
    "# Identify all non-numeric columns in the dataset (factors)\n",
    "\n",
    "non_numeric_list \\<- unlist(lapply(app_train_clean, is.factor)) \\#\n",
    "Create a data table with only non-numeric columns data_non_num \\<-\n",
    "setDT(app_train_clean)\\[,..non_numeric_list\\]\n",
    "\n",
    "# Combine the target variable with the non-numeric data\n",
    "\n",
    "data_non_num \\<- cbind(data_non_num, app_train_clean\\[,‘TARGET’\\])\n",
    "\n",
    "\n",
    "    For clarity and standardization, the variables will be renamed to remove special characters.\n",
    "\n",
    "    ```{r}\n",
    "    # Function to clean the levels of each factor variable\n",
    "    clean_factor_levels <- function(factor_var) {\n",
    "      # Replace spaces and punctuation with underscores and convert to lowercase\n",
    "      levels(factor_var) <- tolower(gsub(\"[[:space:][:punct:]]\", \"_\", levels(factor_var)))\n",
    "      return(factor_var)\n",
    "    }\n",
    "\n",
    "    # Applying the function to each variable in the data.table\n",
    "    data_non_num[] <- lapply(data_non_num, function(x) {\n",
    "      # Check if the variable is a factor\n",
    "      if (is.factor(x)) {\n",
    "        # Clean the factor levels\n",
    "        clean_factor_levels(x)\n",
    "      } else {\n",
    "        # Return the variable unchanged if it is not a factor\n",
    "        return(x)\n",
    "      }\n",
    "    })\n",
    "\n",
    "Dummy encoding is a common method for transforming categorical variables\n",
    "into a numerical format, making them usable for machine learning models.\n",
    "This approach creates binary (0 or 1) columns for each category in a\n",
    "categorical variable, allowing the models to interpret these categories\n",
    "without implying any order. Our intention in using dummy encoding is to\n",
    "avoid biases that can arise when treating categorical variables as\n",
    "ordered. Additionally, dummy encoding helps the model better understand\n",
    "the relationships between the input features and the target variable.\n",
    "\n",
    "``` {r}\n",
    "# Create dummy variables for the non-numeric data, dropping the second level to avoid multicollinearity\n",
    "dummies <- dummyVars(TARGET ~ ., data = data_non_num, drop2nd = TRUE)\n",
    "\n",
    "# Apply the dummy variable transformation to the non-numeric data\n",
    "data_non_num_dum <- predict(dummies, newdata = data_non_num)\n",
    "```\n",
    "\n",
    "The code below combines the variables back into a single data frame\n",
    "after performing dummy encoding. The process will create new columns in\n",
    "the dataset, increasing the total to 253.\n",
    "\n",
    "\\`\\`\\`{r transformation 7} #Function to change index to column\n",
    "index_to_col \\<- function(data, Column_Name){ data \\<- cbind(newColName\n",
    "= rownames(data), data) rownames(data) \\<- 1:nrow(data)\n",
    "colnames(data)\\[1\\] \\<- Column_Name return (data) }\n",
    "\n",
    "# Recreate the list that includes numeric and integer columns\n",
    "\n",
    "numeric_integer_list \\<- unlist(lapply(app_train_clean, function(x)\n",
    "is.numeric(x) \\|\\| is.integer(x)))\n",
    "\n",
    "# Create a new data frame with the numeric and integer columns\n",
    "\n",
    "data_num \\<- setDT(app_train_clean)\\[, ..numeric_integer_list\\] \\#\n",
    "Select only numeric and integer columns\n",
    "\n",
    "# Combine one-hot encoded data with numeric and integer data\n",
    "\n",
    "data_pre \\<- cbind(data_non_num_dum, data_num)\n",
    "\n",
    "\n",
    "    #### 4.2 Handling with Missing Data\n",
    "\n",
    "    The process of identifying missing values in the dataset is described.\n",
    "    The percentage of missing data for each column is calculated and visualized in the following table.\n",
    "    Then, the missing values are replaced with the mean for each variable using the aggregate function.\n",
    "\n",
    "    ```{r transformation 8}\n",
    "    # Calculate the percentage of missing values for each column\n",
    "    mv <- as.data.frame(apply(data_pre, 2, function(col) sum(is.na(col)) / length(col)))\n",
    "    colnames(mv)[1] <- \"missing_values\"  # Rename the first column to \"missing_values\"\n",
    "\n",
    "    # Add a column with the index as the first column\n",
    "    mv <- index_to_col(mv, 'Column')\n",
    "    # Order the missing values in descending order\n",
    "    mv <- setDT(mv)[order(missing_values, decreasing = TRUE)]\n",
    "\n",
    "    # Create an interactive table to display all columns with their missing values percentage\n",
    "    datatable(mv, options = list(pageLength = 10), \n",
    "              caption = \"Percentage of Missing Values for Each Column\") %>%\n",
    "      formatPercentage('missing_values', 1)  # Format the missing values as percentage with 2 decimal points\n",
    "\n",
    "    # Fill in the missing values using aggregation\n",
    "    data_pre <- na.aggregate(data_pre)\n",
    "\n",
    "    # Replace dots with underscores\n",
    "    colnames(data_pre) <- gsub(\"\\\\.\", \"_\", colnames(data_pre))\n",
    "\n",
    "#### 4.3 Sampling and Balancing\n",
    "\n",
    "Due to the high computational cost of processing the entire dataset, we\n",
    "opted to perform downsampling by using nearly all instances that\n",
    "represent TARGET == 1. To balance the dataset and avoid bias in the\n",
    "modeling process, we randomly sampled the same number of instances with\n",
    "TARGET == 0. As a result, the dataset is balanced at 50% for each class,\n",
    "using approximately 23% of the total dataset (72,000 instances) for\n",
    "training and testing. Additionally, We removed the variable SK_ID_CURR\n",
    "because it is not necessary for the modeling process, as it is just a\n",
    "registration number.\n",
    "\n",
    "\\`\\`\\`{r, include=FALSE} \\# Set a seed for reproducibility\n",
    "set.seed(seed)\n",
    "\n",
    "# Remove the SK_ID_CURR variable using subset()\n",
    "\n",
    "data_pre \\<- subset(data_pre, select = -c(SK_ID_CURR))\n",
    "\n",
    "# Subset the data to get all rows with TARGET == 1\n",
    "\n",
    "target_1\\_data \\<- data_pre\\[data_pre\\$TARGET == 1, \\]\n",
    "\n",
    "# Subset the data to get all rows with TARGET == 0\n",
    "\n",
    "target_0\\_data \\<- data_pre\\[data_pre\\$TARGET == 0, \\]\n",
    "\n",
    "# Select samples from each class\n",
    "\n",
    "target_1\\_sample \\<- target_1\\_data\\[sample(nrow(target_1\\_data),\n",
    "24000), \\] target_0\\_sample \\<-\n",
    "target_0\\_data\\[sample(nrow(target_0\\_data), 48000), \\]\n",
    "\n",
    "# Combine the sampled TARGET == 1 and TARGET == 0 data\n",
    "\n",
    "data_pre_sample \\<- rbind(target_1\\_sample, target_0\\_sample)\n",
    "\n",
    "# Shuffle the rows of the combined dataset to mix the classes\n",
    "\n",
    "data_pre_sample \\<- data_pre_sample\\[sample(nrow(data_pre_sample)), \\]\n",
    "\n",
    "# Summarize the distribution of classes\n",
    "\n",
    "summary(as.factor(data_pre_sample\\$TARGET))\n",
    "\n",
    "# Identify the column index of the TARGET variable\n",
    "\n",
    "target_column_index \\<- which(colnames(data_pre_sample) == “TARGET”)\n",
    "print(target_column_index)\n",
    "\n",
    "\n",
    "    **Recursive Feature Elimination for Variable Selection**\n",
    "\n",
    "    This code uses the Recursive Feature Elimination (RFE) method to select important variables from the dataset, aiming to reduce computational time for subsequent analyses.\n",
    "    This process takes many hours to complete, so I chose to save the vector and load it in its final version.\n",
    "\n",
    "    ```{r, message=FALSE, echo=TRUE}\n",
    "\n",
    "    # Register parallel backend for faster computation using available cores\n",
    "    #registerDoParallel(cores = parallel::detectCores() - 1)\n",
    "\n",
    "    #tic()\n",
    "    # Set control parameters for RFE, using random forest functions and cross-validation with 3 folds\n",
    "    #control <- rfeControl(functions=rfFuncs, method=\"cv\", number=3)\n",
    "\n",
    "    # Set training control to compute class probabilities and use AUC as the summary function\n",
    "    #trainctrl <- trainControl(classProbs= TRUE, summaryFunction = twoClassSummary)\n",
    "\n",
    "    # Perform Recursive Feature Elimination (RFE)\n",
    "    #results <- rfe(\n",
    "    #  as.data.frame(data_pre_sample)[, -target_column_index],  # All columns except the target\n",
    "    #  as.data.frame(data_pre_sample)[, target_column_index],   # Only the target\n",
    "    #  rfeControl = control,\n",
    "    #  method = \"rf\",\n",
    "    #  metric = \"AUC\",\n",
    "    #  trControl = trainctrl)\n",
    "\n",
    "    #toc()\n",
    "\n",
    "    # Save the RFE results to an .RData file\n",
    "    #save(results, file = \"D:/mymodels/rfe_results.RData\")\n",
    "    load(\"D:/mymodels/rfe_results.RData\")\n",
    "\n",
    "    # Print the RFE results to see the variable selection performance\n",
    "    #print(results, digits = 2)\n",
    "\n",
    "    # Plot the results showing variable importance and selection performance\n",
    "    plot(results, type=c(\"g\", \"o\"))\n",
    "\n",
    "The results indicate that among the 253 variables in this dataset, fewer\n",
    "than 25 correspond to the lowest root mean squared error. Below is the\n",
    "list of the 10 most important predictors.\n",
    "\n",
    "``` {r}\n",
    "# Obtain the list of predictors ordered by importance\n",
    "selected_predictors <- predictors(results)\n",
    "\n",
    "# Limit to the first 10 predictors\n",
    "top_predictors <- selected_predictors[1:10]\n",
    "\n",
    "# Create a data frame from the top predictors\n",
    "predictors_df <- data.frame(Predictors = top_predictors)\n",
    "\n",
    "# Display the table of ordered predictors\n",
    "knitr::kable(predictors_df, caption = \"Top 10 Predictors\")\n",
    "```\n",
    "\n",
    "To simplify the dataset, we removed the variables\n",
    "NAME_EDUCATION_TYPE_secondary\\_\\_secondary_special,\n",
    "NAME_EDUCATION_TYPE_higher_education,\n",
    "NAME_CONTRACT_TYPE_revolving_loans, and CODE_GENDER_f, as they were\n",
    "redundant or exhibited collinearity, ensuring a cleaner and more\n",
    "effective set of predictors. Additionally, to include a broader range of\n",
    "predictors and improve the model’s quality, we chose to focus on the 50\n",
    "most impactful variables, going beyond the standard Recursive Feature\n",
    "Elimination approach to refine the selection further.\n",
    "\n",
    "\\`\\`\\`{r transformation 9, message=FALSE} \\# Define the number of\n",
    "predictors to keep n_predictors_to_keep \\<- 54\n",
    "\n",
    "# Select the first predictors\n",
    "\n",
    "cols_to_keep \\<- c(predictors(results)\\[1:n_predictors_to_keep\\],\n",
    "“TARGET”)\n",
    "\n",
    "# Create a new data frame that only includes the selected columns\n",
    "\n",
    "data_pre_sample \\<- as.data.frame(data_pre_sample)\\[,\n",
    "(colnames(data_pre_sample) %in% cols_to_keep)\\]\n",
    "\n",
    "# Remove the redundant binary variables from the data frame `data_pre_sample`\n",
    "\n",
    "# Removing the “NAME_EDUCATION_TYPE_secondary\\_\\_\\_secondary_special” column and keeping only “NAME_EDUCATION_TYPE_secondary_special”\n",
    "\n",
    "data_pre_sample\\$NAME_EDUCATION_TYPE_secondary\\_\\_\\_secondary_special\n",
    "\\<- NULL\n",
    "\n",
    "# Removing the “NAME_EDUCATION_TYPE_higher_education” column and keeping only “NAME_EDUCATION_TYPE_higher_edu”\n",
    "\n",
    "data_pre_sample\\$NAME_EDUCATION_TYPE_higher_education \\<- NULL\n",
    "\n",
    "# Removing the “NAME_CONTRACT_TYPE_revolving_loans” column and keeping only “NAME_CONTRACT_TYPE_rev_loans”\n",
    "\n",
    "data_pre_sample\\$NAME_CONTRACT_TYPE_revolving_loans \\<- NULL\n",
    "\n",
    "# Removing the “CODE_GENDER_f” column and keeping only “CODE_GENDER_m”\n",
    "\n",
    "data_pre_sample\\$CODE_GENDER_f \\<- NULL\n",
    "\n",
    "# Remove the variable from data_pre_sample\n",
    "\n",
    "#data_pre_sample \\<- data_pre_sample\\[, !colnames(data_pre_sample) %in%\n",
    "“NAME_CONTRACT_TYPE_Revolving_loan\\_”\\]\n",
    "\n",
    "# Remove the variables ‘NAME_CONTRACT_TYPE_Revolving loans’ and ‘CODE_GENDER_M’ from data_pre_sample\n",
    "\n",
    "#data_pre_sample \\<- data_pre_sample\\[, !colnames(data_pre_sample) %in%\n",
    "c(“NAME_CONTRACT_TYPE_Revolving loans”, “CODE_GENDER_M”)\\]\n",
    "\n",
    "# Save data_pre_sample to a file\n",
    "\n",
    "save(data_pre_sample, file = “D:/mymodels/data_pre_sample.RData”)\n",
    "\n",
    "\n",
    "    #### 4.4 Data partition\n",
    "\n",
    "    **Partition**\n",
    "\n",
    "    The dataset will be partitioned into training and testing sets.\n",
    "    The TARGET variable will be transformed into a binary format to facilitate better model interpretation, and 70% of the data will be allocated for training while the remaining 30% will be reserved for testing, ensuring effective evaluation on unseen data.\n",
    "\n",
    "    ```{r DataPartition}\n",
    "    # Set the seed for reproducibility\n",
    "    set.seed(seed)  \n",
    "\n",
    "    # Transform TARGET variable into a binary factor\n",
    "    data_pre_sample <- mutate(data_pre_sample, TARGET = ifelse(TARGET == 0, 'Class0', 'Class1'))\n",
    "    data_pre_sample$TARGET <- as.factor(data_pre_sample$TARGET)\n",
    "\n",
    "    # Create a partition for training and testing sets; 70% for training\n",
    "    inTrain <- createDataPartition(data_pre_sample$TARGET, p = .7)[[1]]\n",
    "    dt_train <- data_pre_sample[inTrain, ]  # Training data\n",
    "    dt_test  <- data_pre_sample[-inTrain, ]  # Testing data\n",
    "\n",
    "The training control is configured to use repeated cross-validation,\n",
    "enhancing the reliability of the model evaluation. This setup\n",
    "incorporates 10 folds with 2 repeats and enables probability\n",
    "calculations for each class.\n",
    "\n",
    "``` {r}\n",
    "# train control\n",
    "traincntrl <- trainControl(\n",
    "  method = 'repeatedcv',          # Use repeated cross-validation as the method\n",
    "  number = 10,                     # Number of folds for cross-validation \n",
    "  repeats = 2,                    # Number of times to repeat the cross-validation process \n",
    "  classProbs = TRUE,              # Calculate class probabilities to evaluate performance\n",
    "  summaryFunction = twoClassSummary, # Use the twoClassSummary to evaluate performance metrics\n",
    "  verboseIter = FALSE,             # Show progress during training\n",
    "  allowParallel = TRUE)           # Allow parallel processing for faster training\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### 5. Modeling\n",
    "\n",
    "Due to the computational cost of the modeling process, we opted to save\n",
    "the final versions of the models, making the knitting process easier.\n",
    "\n",
    "All the metrics displayed within the model blocks refer to their\n",
    "performance on the test set (30%, 21,600 instances).\n",
    "\n",
    "#### 5.1 Logistic Regression\n",
    "\n",
    "Since we have already performed the processes of variable elimination\n",
    "and standardization, I decided not to use techniques like Lasso and\n",
    "Ridge at this moment. (Note: we tested them and still opted to remove\n",
    "it.)\n",
    "\n",
    "\\`\\`\\`{r, message=FALSE, warning=FALSE} \\# Set time measure tic()\n",
    "\n",
    "# Train the logistic regression model with centering and scaling\n",
    "\n",
    "logistic_model \\<- train(TARGET \\~ ., \\# Train the model to predict\n",
    "TARGET using all predictors data = dt_train, \\# Use the training dataset\n",
    "method = ‘glm’, \\# Specify the method as generalized linear model\n",
    "preProcess = c(“center”, “scale”), \\# Apply centering and scaling\n",
    "trControl = traincntrl) \\# Apply the training control settings \\# End\n",
    "time measure toc()\n",
    "\n",
    "# Save the logistic regression model to an .RData file\n",
    "\n",
    "#save(logistic_model, file = “D:/mymodels/logistic_model.RData”)\n",
    "#load(“D:/mymodels/logistic_model.RData”)\n",
    "\n",
    "#Summary print(summary(logistic_model), digits = 2)\n",
    "\n",
    "\n",
    "    The coefficient values in the logistic regression are expressed as probabilities of occurrence, indicating the likelihood of an event happening for each predictor. However, interpreting these coefficients can be complex, as they are influenced by the scale and type of variables used in the model, with only 20 out of 50 predictors meeting the 5% significance level or lower. \n",
    "\n",
    "    These include: NAME_CONTRACT_TYPE_cash_loans, CODE_GENDER_m, AMT_INCOME_TOTAL, AMT_CREDIT, AMT_ANNUITY, AMT_GOODS_PRICE, DAYS_ID_PUBLISH, OWN_CAR_AGE, REGION_RATING_CLIENT_W_CITY, REG_CITY_NOT_LIVE_CITY, EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3, DEF_30_CNT_SOCIAL_CIRCLE, DAYS_LAST_PHONE_CHANGE, FLAG_DOCUMENT_6, DAYS_EMPLOYED_YEARS, DAYS_REGISTRATION, YEARS_BUILD_AVG, and YEARS_BUILD_MEDI.\n",
    "\n",
    "    Among these 20 predictors, 8 are listed among the top 10 most important predictors identified through the RFE process.\n",
    "\n",
    "    **Train set Metrics**\n",
    "\n",
    "    ```{r, message=FALSE, warning=FALSE}\n",
    "    # Predict probabilities on the train dataset\n",
    "    pred_train_logistic <- predict(logistic_model, newdata = dt_train, type = \"prob\")  # Generate predicted probabilities for each class\n",
    "\n",
    "    # ROC and AUC for the train set\n",
    "    roc_train_logistic <- roc(dt_train$TARGET, pred_train_logistic$Class1, levels = c(\"Class0\", \"Class1\"), direction = \"<\")\n",
    "    auc_train_logistic <- auc(roc_train_logistic)  # Calculate AUC\n",
    "    cat(\"AUC Train set - Logistic:\", round(auc_train_logistic, 2), \"\\n\")\n",
    "\n",
    "    # Predict class labels for the train set using the logistic regression model\n",
    "    predictClassesTrainLogistic <- predict(logistic_model, dt_train)\n",
    "\n",
    "    # Calculate Accuracy for the train set\n",
    "    accuracy_train_logistic <- mean(predictClassesTrainLogistic == dt_train$TARGET)\n",
    "    cat(\"Accuracy Train set - Logistic:\", round(accuracy_train_logistic, 2), \"\\n\")\n",
    "\n",
    "    # Generate confusion matrix for the logistic regression model on the train set \n",
    "    conf_matrix_train_logistic <- confusionMatrix(data = predictClassesTrainLogistic,\n",
    "                                                  reference = dt_train$TARGET,\n",
    "                                                  positive = \"Class1\")  # \"Class1\" as the positive class\n",
    "\n",
    "The model showed the above performance on the train set.\n",
    "\n",
    "**AUC - Logistic Model - Test set**\n",
    "\n",
    "\\`\\`\\`{r,message=FALSE, warning=FALSE} \\# Predict probabilities on the\n",
    "test dataset pred_test_logistic \\<- predict(logistic_model, newdata =\n",
    "dt_test, type = “prob”) \\# Generate predicted probabilities for each\n",
    "class\n",
    "\n",
    "# Calculate the ROC and AUC for the test set (with “Class1” as the positive class)\n",
    "\n",
    "roc_test_logistic \\<- roc(dt_test$TARGET, pred_test_logistic$Class1,\n",
    "levels = c(“Class0”, “Class1”), direction = “\\<”) \\# Create ROC object\n",
    "\n",
    "# Calculate AUC\n",
    "\n",
    "auc_test_logistic \\<- auc(roc_test_logistic) \\# Calculate AUC cat(“AUC\n",
    "Test set - Logistic:”, round(auc_test_logistic, 2), “”) \\# Print the AUC\n",
    "value for the test set\n",
    "\n",
    "# Plot the ROC curve for the test set\n",
    "\n",
    "plot(roc_test_logistic, col = “blue”, lwd = 2, main = “ROC Curve -\n",
    "Logistic Regression (Test Set)”) \\# Plot ROC curve\n",
    "\n",
    "\n",
    "    **Confusion Matrix - Logistic Regression Test Set**\n",
    "\n",
    "    ```{r,message=FALSE, warning=FALSE}\n",
    "    # Predict class labels for the test set using the logistic regression model\n",
    "    predictClassesTestLogistic <- predict(logistic_model, dt_test)\n",
    "\n",
    "    # Calculate the confusion matrix for the logistic regression model on the test set\n",
    "    conf_matrix_test_logistic <- confusionMatrix(data = predictClassesTestLogistic,\n",
    "                                                 reference = dt_test$TARGET,\n",
    "                                                 positive = \"Class1\")  # Specify \"Class1\" as the positive class\n",
    "\n",
    "    # Display the confusion matrix in the console\n",
    "    print(conf_matrix_test_logistic)\n",
    "\n",
    "Results for the logistic model (test set) include an AUC of 0.74,\n",
    "accuracy of 71%, sensitivity (true positive for Class 1) of 37%, and\n",
    "specificity (true negative for Class 0) of 89%. The logistic regression\n",
    "model provides a reasonable baseline for distinguishing between\n",
    "defaulters and non-defaulters, being better than a classification based\n",
    "on the no-information rate.\n",
    "\n",
    "#### 5.2 Random Forest\n",
    "\n",
    "In the Random Forest model setup, I had to make several adjustments as\n",
    "the model was showing signs of overfitting to the training set. Key\n",
    "hyperparameters included experimenting with different values for the\n",
    "number of features considered at each split to balance model complexity\n",
    "and accuracy. I also explored different split criteria and modified the\n",
    "minimum node size to control tree depth and reduce overfitting. The\n",
    "model was trained using repeated cross-validation with 10 folds and 2\n",
    "repeats. Class probabilities were calculated for performance evaluation,\n",
    "and parallel processing was enabled to speed up training. Also I used\n",
    "the “ranger” method for Random Forest modeling because it’s efficient\n",
    "with large datasets and faster than traditional implementations, plus it\n",
    "supports parallel processing.\n",
    "\n",
    "\\`\\`\\`{r, message=FALSE} \\# Set time measure #tic()\n",
    "\n",
    "#random_forest_model \\<- train( \\# x = setDT(dt_train)\\[,\n",
    "-c(‘TARGET’)\\], \\# Remove the target variable from the features \\# y =\n",
    "dt_train\\$TARGET, \\# Define the target variable \\# method = ‘ranger’, \\#\n",
    "Using the ranger package for Random Forest \\# preProc = c(‘center’,\n",
    "‘scale’), \\# Standardize the data by centering and scaling \\# tuneGrid =\n",
    "expand.grid( \\# .mtry = c(2, 4, 6), \\# Number of features considered at\n",
    "each split \\# .splitrule = c(“gini”, “extratrees”), \\# Experiment with\n",
    "different split criteria \\# .min.node.size = c(50, 150, 250)), \\# Adjust\n",
    "to reduce tree complexity (minimum node size)\n",
    "\n",
    "# metric = “ROC”, \\# Evaluate the model using the ROC metric\n",
    "\n",
    "# trControl = traincntrl, \\# Use cross-validation for model evaluation\n",
    "\n",
    "# num.trees = 120, \\# Number of trees\n",
    "\n",
    "# importance = “impurity”) \\# Request calculation of variable importance\n",
    "\n",
    "# End time measure\n",
    "\n",
    "#toc()\n",
    "\n",
    "# Save the random forest model to an .RData file\n",
    "\n",
    "#save(random_forest_model, file =\n",
    "“D:/mymodels/random_forest_model.RData”)\n",
    "load(“D:/mymodels/random_forest_model.RData”)\n",
    "\n",
    "# View the best hyperparameters\n",
    "\n",
    "best_hyperparameters \\<- random_forest_model\\$bestTune\n",
    "print(best_hyperparameters)\n",
    "\n",
    "    The best hyperparameters obtained for the model are: mtry = 15, which means that 15 variables were selected for each split; splitrule = \"gini\", indicating that the splits were made based on the Gini index; and min.node.size = 250, which sets the minimum number of observations required in each node before it can be split.\n",
    "\n",
    "    ```{r}\n",
    "    # Extract the variable importance values from the trained model\n",
    "    importance_rf <- varImp(random_forest_model, scale = FALSE)\n",
    "\n",
    "    # Normalize importance values between 0 and 1\n",
    "    importance_rf$importance <- importance_rf$importance / max(importance_rf$importance)\n",
    "\n",
    "    # Plot normalized importance of the top 10 most important variables\n",
    "    plot(importance_rf, top = 10, main = \"Random Forest Top 10 Most Important Variables (Normalized)\")\n",
    "\n",
    "These are the top 10 most important variables in the model, with\n",
    "EXT_SOURCE_3 being the most important, followed by EXT_SOURCE_2 and\n",
    "EXT_SOURCE_1. The other variables, such as DAYS_EMPLOYED_YEARS and\n",
    "DAYS_BIRTH, have progressively lower importance in comparison.\n",
    "\n",
    "Among these 10 predictors, 9 are listed among the top 10 most important\n",
    "predictors identified through the RFE process.\n",
    "\n",
    "**Train set Metrics**\n",
    "\n",
    "``` {r}\n",
    "# Predict probabilities on the train dataset using the random forest model\n",
    "pred_train_rf <- predict(random_forest_model, newdata = dt_train, type = \"prob\")  # Generate predicted probabilities for each class\n",
    "\n",
    "# ROC and AUC for the train set using the random forest model\n",
    "roc_train_rf <- roc(dt_train$TARGET, pred_train_rf$Class1, levels = c(\"Class0\", \"Class1\"), direction = \"<\")\n",
    "auc_train_rf <- auc(roc_train_rf)  # Calculate AUC\n",
    "cat(\"AUC Train set - Random Forest:\", round(auc_train_rf, 2), \"\\n\")\n",
    "\n",
    "# Predict class labels for the train set using the random forest model\n",
    "predictClassesTrainRF <- predict(random_forest_model, dt_train)\n",
    "\n",
    "# Calculate Accuracy for the train set using the random forest model\n",
    "accuracy_train_rf <- mean(predictClassesTrainRF == dt_train$TARGET)\n",
    "cat(\"Accuracy Train set - Random Forest:\", round(accuracy_train_rf, 2), \"\\n\")\n",
    "\n",
    "# Generate confusion matrix for the random forest model on the train set \n",
    "conf_matrix_train_rf <- confusionMatrix(data = predictClassesTrainRF,\n",
    "                                        reference = dt_train$TARGET,\n",
    "                                        positive = \"Class1\")  # \"Class1\" as the positive class\n",
    "```\n",
    "\n",
    "Initially with maximum overfitting to the train set, the model now has\n",
    "better balance, aiming for greater generalization.\n",
    "\n",
    "**AUC - Random Forest - Test Set**\n",
    "\n",
    "\\`\\`\\`{r,message=FALSE, warning=FALSE} \\# Predict probabilities on the\n",
    "test set using the random forest model pred_test_rf \\<-\n",
    "predict(random_forest_model, newdata = dt_test, type = “prob”) \\#\n",
    "Generate predicted probabilities for each class\n",
    "\n",
    "# Calculate the ROC and AUC for the test set using the random forest model (with “Class1” as the positive class)\n",
    "\n",
    "roc_test_rf \\<- roc(dt_test$TARGET, pred_test_rf$Class1, levels =\n",
    "c(“Class0”, “Class1”), direction = “\\<”) \\# Create ROC object\n",
    "\n",
    "# Plot the ROC curve for the test set\n",
    "\n",
    "plot(roc_test_rf, col = “blue”, lwd = 2, main = “ROC Curve - Random\n",
    "Forest (Test Set)”) \\# Plot ROC curve\n",
    "\n",
    "# Calculate AUC for the test set\n",
    "\n",
    "auc_test_rf \\<- auc(roc_test_rf)\n",
    "\n",
    "# Display AUC in the console\n",
    "\n",
    "cat(“AUC Test set - Random Forest:”, round(auc_test_rf, 2), “”) \\# Print\n",
    "the AUC value\n",
    "\n",
    "\n",
    "    **Confusion Matrix - Random Forest - Test Set**\n",
    "\n",
    "    ```{r}\n",
    "    # Predict class labels for the test set using the random forest model\n",
    "    predictClassesTestRF <- predict(random_forest_model, dt_test)\n",
    "\n",
    "    # Calculate the confusion matrix for the random forest model on the test set\n",
    "    conf_matrix_test_rf <- confusionMatrix(data = predictClassesTestRF,\n",
    "                                           reference = dt_test$TARGET,\n",
    "                                           positive = \"Class1\")  # Specify \"Class1\" as the positive class\n",
    "\n",
    "    # Display the confusion matrix in the console\n",
    "    print(conf_matrix_test_rf)\n",
    "\n",
    "Results for the Random Forest model (test set) include an AUC of 0.74,\n",
    "accuracy of 72%, sensitivity (true positive for Class 1) of 32%, and\n",
    "specificity (true negative for Class 0) of 92%. The Random Forest model\n",
    "offers a slight improvement in specificity compared to logistic\n",
    "regression, making it more effective at identifying non-defaulters.\n",
    "However, its lower sensitivity means it may miss more defaulters. While\n",
    "less interpretable than logistic regression, Random Forest can capture\n",
    "non-linear relationships and interactions among variables, which is\n",
    "useful for complex datasets, but it does not show a significant\n",
    "performance gain in this case.\n",
    "\n",
    "#### 5.3 XGBoost Model\n",
    "\n",
    "The XGBoost model was optimized in multiple stages to improve\n",
    "computational performance. The dataset was split into training and test\n",
    "sets, then transformed into the xgb.DMatrix format. Hyperparameter\n",
    "tuning was performed in rounds, adjusting key parameters like max_depth,\n",
    "min_child_weight, and eta to maximize the AUC score. The second round\n",
    "fine-tuned sampling parameters (subsample, colsample_bytree) to reduce\n",
    "overfitting, followed by regularization parameters (gamma, lambda,\n",
    "alpha) in the final round.\n",
    "\n",
    "Finally, the number of boosting iterations (nrounds) was optimized, with\n",
    "cross-validation used throughout to identify the best parameter\n",
    "combination. This process improved performance while minimizing\n",
    "overfitting.\n",
    "\n",
    "\\`\\`\\`{r, message=FALSE, warning=FALSE} \\# Prepare data for XGBoost\n",
    "(separated from training and test datasets) dtrain_xgb \\<- xgb.DMatrix(\n",
    "data = as.matrix(select(dt_train, -TARGET)), label =\n",
    "as.numeric(dt_train\\$TARGET) - 1) \\# Convert the target to binary (0 and\n",
    "1)\n",
    "\n",
    "dtest_xgb \\<- xgb.DMatrix( data = as.matrix(select(dt_test, -TARGET)),\n",
    "label = as.numeric(dt_test\\$TARGET) - 1) \\# Same conversion for the test\n",
    "set\n",
    "\n",
    "# Set time measure\n",
    "\n",
    "#tic()\n",
    "\n",
    "# Function to evaluate model parameters using cross-validation\n",
    "\n",
    "evaluate_params \\<- function(params, dtrain, max_trees = 150) {\n",
    "cv_results \\<- xgb.cv( params = params, data = dtrain, nrounds =\n",
    "max_trees, nfold = 10,  \n",
    "early_stopping_rounds = 50, verbose = 0)\n",
    "\n",
    "best_auc \\<- max(cv_results$evaluation_log$test_auc_mean) \\# Use AUC for\n",
    "evaluation best_nround \\<-\n",
    "which.max(cv_results$evaluation_log$test_auc_mean) return(list(auc =\n",
    "best_auc, nrounds = best_nround)) }\n",
    "\n",
    "# Initial parameters regularization to avoid overfitting\n",
    "\n",
    "base_params \\<- list( objective = “binary:logistic”, \\# Binary\n",
    "classification eval_metric = “auc”, \\# AUC is more robust for binary\n",
    "classification, especially with imbalanced data tree_method = “hist”, \\#\n",
    "Efficient tree building for large datasets max_depth = 5, \\# Limited\n",
    "tree depth to prevent overfitting min_child_weight = 5, \\# Minimum\n",
    "number of data points required in a child node eta = 0.05, \\# Learning\n",
    "rate to prevent large steps in training gamma = 0.1, \\# Regularization\n",
    "parameter for pruning lambda = 0.1, \\# L2 regularization (shrinkage of\n",
    "leaf weights) alpha = 0.1, \\# L1 regularization (lasso) subsample = 0.8,\n",
    "\\# Random sample of training data for each tree (helps reduce\n",
    "overfitting) colsample_bytree = 0.8) \\# Random sampling of features for\n",
    "each tree\n",
    "\n",
    "# Hyperparameter tuning for basic tree parameters (max_depth, min_child_weight, eta)\n",
    "\n",
    "tune_round1 \\<- function(dtrain) { tune_grid \\<- expand.grid( max_depth\n",
    "= c(3, 5, 7), \\# Trying different tree depths min_child_weight = c(20,\n",
    "40, 60), eta = c(0.05, 0.1, 0.5))\n",
    "\n",
    "best_params \\<- list() best_auc \\<- 0 best_nround \\<- 0\n",
    "\n",
    "for (i in 1:nrow(tune_grid)) { params \\<- list( objective =\n",
    "“binary:logistic”, eval_metric = “auc”, max_depth =\n",
    "tune_grid$max_depth[i],  min_child_weight = tune_grid$min_child_weight\\[i\\],\n",
    "eta = tune_grid\\$eta\\[i\\])\n",
    "\n",
    "    result <- evaluate_params(params, dtrain)\n",
    "    if (result$auc > best_auc) {\n",
    "      best_auc <- result$auc\n",
    "      best_params <- params\n",
    "      best_nround <- result$nrounds } }\n",
    "\n",
    "return(list(params = best_params, nrounds = best_nround, auc =\n",
    "best_auc)) }\n",
    "\n",
    "# Tuning for sampling parameters (subsample and colsample_bytree)\n",
    "\n",
    "tune_round2 \\<- function(base_params, dtrain) { tune_grid \\<-\n",
    "expand.grid( subsample = seq(0.6, 1, 0.1), colsample_bytree = seq(0.6,\n",
    "1, 0.1))\n",
    "\n",
    "best_params \\<- base_params best_auc \\<- 0 best_nround \\<- 0\n",
    "\n",
    "for (i in 1:nrow(tune_grid)) { current_params \\<- base_params\n",
    "current_params$subsample <- tune_grid$subsample\\[i\\]\n",
    "current_params$colsample_bytree <- tune_grid$colsample_bytree\\[i\\]\n",
    "result \\<- evaluate_params(current_params, dtrain) if\n",
    "(result$auc > best_auc) {  best_auc <- result$auc best_params \\<-\n",
    "current_params best_nround \\<- result\\$nrounds } }\n",
    "\n",
    "return(list(params = best_params, nrounds = best_nround, auc =\n",
    "best_auc)) }\n",
    "\n",
    "# Fine-tune the regularization parameters (gamma, lambda, alpha)\n",
    "\n",
    "tune_round3 \\<- function(base_params, dtrain) { tune_grid \\<-\n",
    "expand.grid( gamma = c(0, 0.1, 0.3, 0.5, 1), lambda = c(0.01, 0.1, 1),\n",
    "alpha = c(0.01, 0.1, 1))\n",
    "\n",
    "best_params \\<- base_params best_auc \\<- 0 best_nround \\<- 0\n",
    "\n",
    "for (i in 1:nrow(tune_grid)) { current_params \\<- base_params\n",
    "current_params$gamma <- tune_grid$gamma\\[i\\]\n",
    "current_params$lambda <- tune_grid$lambda\\[i\\]\n",
    "current_params$alpha <- tune_grid$alpha\\[i\\] result \\<-\n",
    "evaluate_params(current_params, dtrain) if\n",
    "(result$auc > best_auc) {  best_auc <- result$auc best_params \\<-\n",
    "current_params best_nround \\<- result\\$nrounds } }\n",
    "\n",
    "return(list(params = best_params, nrounds = best_nround, auc =\n",
    "best_auc)) }\n",
    "\n",
    "# Fine-tune the number of trees by testing around the optimal number of rounds\n",
    "\n",
    "fine_tune_trees \\<- function(params, dtrain, current_nround) {\n",
    "test_rounds \\<- seq(max(1, current_nround - 100), current_nround + 100,\n",
    "by = 50) best_auc \\<- 0 best_nround \\<- current_nround\n",
    "\n",
    "for (n in test_rounds) { cv_results \\<- xgb.cv( params = params, data =\n",
    "dtrain, nrounds = n, nfold = 5, \\# Reduced folds for faster execution\n",
    "early_stopping_rounds = 50, verbose = 0)\n",
    "\n",
    "    current_auc <- max(cv_results$evaluation_log$test_auc_mean)  # Use AUC for evaluation\n",
    "    if (current_auc > best_auc) {\n",
    "      best_auc <- current_auc\n",
    "      best_nround <- n } }\n",
    "\n",
    "return(list(params = params, nrounds = best_nround, auc = best_auc))}\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "\n",
    "final_results \\<- fine_tune_trees(params = base_params, dtrain =\n",
    "dtrain_xgb, current_nround = 150)\n",
    "\n",
    "xgboost_model \\<- xgb.train( params =\n",
    "final_results$params, # Best parameters from tuning  data = dtrain_xgb,  nrounds = final_results$nrounds,\n",
    "watchlist = list(train = dtrain_xgb, test = dtest_xgb), verbose = 0)\n",
    "\n",
    "# End time measure\n",
    "\n",
    "toc()\n",
    "\n",
    "# Save the XGBoost Faster model in a single .RData file in D: drive\n",
    "\n",
    "save(xgboost_model, file = “D:/mymodels/xgboost_model.RData”)\n",
    "#load(“D:/mymodels/xgboost_model_faster.RData”)\n",
    "\n",
    "\n",
    "    **Best hyperparameters and Predictors**\n",
    "\n",
    "    ```{r}\n",
    "    # Displaying Final Model Details\n",
    "    #cat(\"XGBoost Model Summary:\\n\")\n",
    "    #cat(\"Number of Trees:\", final_results$nrounds, \"\\n\")  # Retrieve the number of trees from the model\n",
    "\n",
    "    # Variable Importance\n",
    "    importance <- xgb.importance(\n",
    "      feature_names = colnames(select(dt_train, -TARGET)),  # Exclude the TARGET variable from the feature list\n",
    "      model = xgboost_model  # Use the trained XGBoost model to extract feature importance\n",
    "    )\n",
    "\n",
    "    # Format the importance values to two decimal places for better readability\n",
    "    importance[, `:=`(Gain = round(Gain, 2), Cover = round(Cover, 2), Frequency = round(Frequency, 2))]\n",
    "\n",
    "    # Create an interactive table to display all the predictors and their importance values\n",
    "    datatable(importance, options = list(pageLength = 10), \n",
    "              caption = \"XGBoost Feature Importance\") %>%\n",
    "      formatPercentage('Gain', 2) %>%  # Format the 'Gain' values as percentages with 2 decimal places\n",
    "      formatPercentage('Cover', 2) %>%  # Format the 'Cover' values as percentages with 2 decimal places\n",
    "      formatPercentage('Frequency', 2)  # Format the 'Frequency' values as percentages with 2 decimal places\n",
    "\n",
    "    # Normalize the feature importances to a scale of 0 to 1 for the Gain column\n",
    "    importance$NormalizedGain <- importance$Gain / max(importance$Gain)\n",
    "\n",
    "    # Select the top 10 most important features based on the normalized Gain\n",
    "    top_10_importance <- importance[order(-NormalizedGain)][1:10]\n",
    "\n",
    "    # Plot Variable Importance for the top 10 features\n",
    "    ggplot(top_10_importance, aes(x = reorder(Feature, NormalizedGain), y = NormalizedGain)) +\n",
    "      geom_bar(stat = \"identity\", fill = \"lightblue\") +\n",
    "      coord_flip() +\n",
    "      labs(title = \"XGBoost Top 10 Most Important Features (Normalized)\", \n",
    "           x = \"Features\", \n",
    "           y = \"Normalized Gain\") +\n",
    "      theme_minimal() +\n",
    "      theme(panel.grid.major.y = element_blank(),  # Remove horizontal grid lines\n",
    "            panel.grid.minor.y = element_blank())  # Remove minor horizontal grid lines\n",
    "\n",
    "The final model was trained with 250 trees, and the AUC metric was used\n",
    "to evaluate its performance. The feature importance analysis revealed\n",
    "that EXT_SOURCE_3 and EXT_SOURCE_2 are the most relevant variables, with\n",
    "contributions of 24% and 23%, respectively.\n",
    "\n",
    "Additionally, “Cover” and “Frequency” metrics provide insight into how\n",
    "these features contribute to the model. “Cover” measures the proportion\n",
    "of samples that each feature influences, indicating how broadly the\n",
    "feature affects the data. “Frequency” shows how often each feature is\n",
    "used to split the data in the decision trees. EXT_SOURCE_3 and\n",
    "EXT_SOURCE_2 not only have high contributions to the model but also\n",
    "exhibit higher cover and frequency, meaning they are both widely and\n",
    "frequently utilized in the model’s decision-making process.\n",
    "\n",
    "Other important variables include EXT_SOURCE_1, DAYS_EMPLOYED_YEARS, and\n",
    "DAYS_BIRTH, which also play significant roles in the model’s\n",
    "predictions. These variables help model the relationship between\n",
    "customer characteristics and the likelihood of success in classifying\n",
    "the TARGET variable.\n",
    "\n",
    "Only OWN_CAR_AGE and NAME_EDUCATION_TYPE_higher_education are not among\n",
    "the top 10 most important variables from the RFE.\n",
    "\n",
    "**Train Set Metrics**\n",
    "\n",
    "``` {r}\n",
    "# Predict probabilities on the train dataset using the XGBoost model\n",
    "pred_train_xgb <- predict(xgboost_model, newdata = dtrain_xgb)  # Generate predicted probabilities for each class\n",
    "\n",
    "# ROC and AUC for the train set using the XGBoost model\n",
    "roc_train_xgb <- roc(dt_train$TARGET, pred_train_xgb, levels = c(\"Class0\", \"Class1\"), direction = \"<\")  # ROC object for XGBoost\n",
    "auc_train_xgb <- auc(roc_train_xgb)  # Calculate AUC\n",
    "cat(\"AUC Train set - XGBoost:\", round(auc_train_xgb, 2), \"\\n\")\n",
    "\n",
    "# Predict class labels for the train set using the XGBoost model\n",
    "predictClassesTrainXGB <- ifelse(pred_train_xgb > 0.5, \"Class1\", \"Class0\")  # Convert probabilities to class labels\n",
    "\n",
    "# Calculate Accuracy for the train set using the XGBoost model\n",
    "accuracy_train_xgb <- mean(predictClassesTrainXGB == dt_train$TARGET)\n",
    "cat(\"Accuracy Train set - XGBoost:\", round(accuracy_train_xgb, 2), \"\\n\")\n",
    "\n",
    "# Generate confusion matrix for the XGBoost model on the train set\n",
    "conf_matrix_train_xgb <- confusionMatrix(data = factor(predictClassesTrainXGB, levels = c(\"Class0\", \"Class1\")),\n",
    "                                         reference = dt_train$TARGET,\n",
    "                                         positive = \"Class1\")  # \"Class1\" as the positive class\n",
    "```\n",
    "\n",
    "**AUC - XGBoost Model**\n",
    "\n",
    "``` {r}\n",
    "# Predict class probabilities on the test dataset using the XGBoost model\n",
    "pred_test_xgb <- predict(xgboost_model, newdata = dtest_xgb)  # Generate predicted probabilities for each class\n",
    "\n",
    "# For binary classification, probabilities for Class1\n",
    "pred_test_xgb_class1 <- pred_test_xgb  # Probability for Class 1\n",
    "pred_test_xgb_class0 <- 1 - pred_test_xgb_class1  # Probability for Class 0\n",
    "\n",
    "# Calculate ROC and AUC for the test set using the XGBoost model\n",
    "roc_test_xgb <- roc(dt_test$TARGET, pred_test_xgb_class1, levels = c(\"Class0\", \"Class1\"), direction = \"<\")  # Create ROC object\n",
    "\n",
    "# Plot the ROC curve\n",
    "plot(roc_test_xgb, col = \"blue\", lwd = 2, main = \"ROC Curve - XGBoost Model\")  # Plot ROC curve\n",
    "\n",
    "# Calculate and display the AUC value in the console\n",
    "auc_test_xgb <- auc(roc_test_xgb)\n",
    "cat(\"AUC Test set - XGBoost:\", auc_test_xgb, \"\\n\")  # Print the AUC value\n",
    "```\n",
    "\n",
    "**Confusion Matrix - XGBoost Model**\n",
    "\n",
    "``` {r}\n",
    "# Predict class labels on the test dataset using the trained XGBoost model\n",
    "predictClassesTestXGB <- ifelse(pred_test_xgb_class1 > 0.5, \"Class1\", \"Class0\")  # Convert probabilities to class labels\n",
    "\n",
    "# Calculate the confusion matrix to evaluate the XGBoost model's performance\n",
    "conf_matrix_test_xgb <- confusionMatrix(data = factor(predictClassesTestXGB, levels = c(\"Class0\", \"Class1\")),  # Predicted class labels\n",
    "                                        reference = dt_test$TARGET,        # True class labels from the test dataset\n",
    "                                        positive = \"Class1\")               # Specify the positive class as \"Class1\"\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(conf_matrix_test_xgb)\n",
    "```\n",
    "\n",
    "Results for the XGBoost model (test set) include an AUC of 0.75,\n",
    "accuracy of 73%, sensitivity of 42%, and specificity of 88%. The XGBoost\n",
    "model outperforms both logistic regression and Random Forest, offering\n",
    "the highest AUC and accuracy with a balanced performance in sensitivity\n",
    "and specificity. XGBoost’s ability to capture complex patterns and\n",
    "interactions likely contributed to this improved performance.\n",
    "Additionally, the modeling process was much faster than with Random\n",
    "Forest, given the way it was implemented.\n",
    "\n",
    "#### 5.4 LightGBM Model\n",
    "\n",
    "The LightGBM model was optimized by tuning key parameters in multiple\n",
    "stages. First, the dataset was split into training and test sets and\n",
    "converted to the lgb.Dataset format. The model was then fine-tuned by\n",
    "adjusting tree parameters (max_depth, min_data_in_leaf, learning_rate),\n",
    "followed by sampling and regularization parameters to minimize\n",
    "overfitting. Cross-validation was used to determine the optimal number\n",
    "of boosting iterations. The training process was faster than Random\n",
    "Forest, showcasing LightGBM’s efficiency while maintaining strong\n",
    "performance.\n",
    "\n",
    "\\`\\`\\`{r, message=FALSE, warning=FALSE} \\# Load the data_pre_sample\n",
    "dataset from the specified file\n",
    "load(“D:/mymodels/data_pre_sample.RData”)\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "\n",
    "set.seed(seed)\n",
    "\n",
    "# Create a partition for training and testing sets; 70% for training\n",
    "\n",
    "inTrain \\<- createDataPartition(data_pre_sample\\$TARGET, p = 0.7, list =\n",
    "FALSE)\n",
    "\n",
    "# Create the training and testing subsets\n",
    "\n",
    "dt_train \\<- data_pre_sample\\[inTrain, \\] dt_test \\<-\n",
    "data_pre_sample\\[-inTrain, \\]\n",
    "\n",
    "# Convert data to lgb.Dataset\n",
    "\n",
    "dtrain_lgb \\<- lgb.Dataset(data = as.matrix(dt_train\\[,\n",
    "-which(names(dt_train) == “TARGET”)\\]), label = dt_train\\$TARGET)\n",
    "\n",
    "dtest_lgb \\<- lgb.Dataset(data = as.matrix(dt_test\\[,\n",
    "-which(names(dt_test) == “TARGET”)\\]), label = dt_test\\$TARGET,\n",
    "free_raw_data = FALSE)\n",
    "\n",
    "# Set time measure\n",
    "\n",
    "tic()\n",
    "\n",
    "# Base parameters for lightgbm\n",
    "\n",
    "base_params_lgb \\<- list( objective = “binary”, metric = “binary_error”,\n",
    "boosting_type = “gbdt”, num_threads = 4)\n",
    "\n",
    "# Initial model evaluation: cross-validation to find best nrounds\n",
    "\n",
    "evaluate_params_lgb \\<- function(params, dtrain, max_trees = 2000) {\n",
    "cv_results \\<- lgb.cv( params = params, data = dtrain, nrounds =\n",
    "max_trees, nfold = 10, early_stopping_rounds = 50, verbose = -1)\n",
    "\n",
    "best_rmse \\<- min(cv_results$evaluation_log$valid_rmse) best_nround \\<-\n",
    "which.min(cv_results$evaluation_log$valid_rmse) return(list(rmse =\n",
    "best_rmse, nrounds = best_nround)) }\n",
    "\n",
    "# Tune basic tree parameters\n",
    "\n",
    "tune_round1_lgb \\<- function(dtrain) { tune_grid \\<- expand.grid(\n",
    "max_depth = c(3, 5, 7), min_data_in_leaf = c(10, 20, 30), learning_rate\n",
    "= c(0.01, 0.05, 0.1))\n",
    "\n",
    "best_params \\<- list() best_rmse \\<- Inf best_nround \\<- 0\n",
    "\n",
    "for (i in 1:nrow(tune_grid)) { params \\<- list( objective = “binary”,\n",
    "metric = “binary_error”, max_depth =\n",
    "tune_grid$max_depth[i],  min_data_in_leaf = tune_grid$min_data_in_leaf\\[i\\],\n",
    "learning_rate = tune_grid\\$learning_rate\\[i\\])\n",
    "\n",
    "    result <- evaluate_params_lgb(params, dtrain)\n",
    "    if (result$rmse < best_rmse) {\n",
    "      best_rmse <- result$rmse\n",
    "      best_params <- params\n",
    "      best_nround <- result$nrounds } }\n",
    "\n",
    "return(list(params = best_params, nrounds = best_nround, rmse =\n",
    "best_rmse))}\n",
    "\n",
    "# Tune sampling parameters\n",
    "\n",
    "tune_round2_lgb \\<- function(base_params, dtrain) { tune_grid \\<-\n",
    "expand.grid( subsample = seq(0.6, 1, 0.1), colsample_bytree = seq(0.6,\n",
    "1, 0.1))\n",
    "\n",
    "best_params \\<- base_params best_rmse \\<- Inf best_nround \\<- 0\n",
    "\n",
    "for (i in 1:nrow(tune_grid)) { current_params \\<- base_params\n",
    "current_params$subsample <- tune_grid$subsample\\[i\\]\n",
    "current_params$colsample_bytree <- tune_grid$colsample_bytree\\[i\\]\n",
    "result \\<- evaluate_params_lgb(current_params, dtrain) if\n",
    "(result$rmse < best_rmse) {  best_rmse <- result$rmse best_params \\<-\n",
    "current_params best_nround \\<- result\\$nrounds } }\n",
    "\n",
    "return(list(params = best_params, nrounds = best_nround, rmse =\n",
    "best_rmse)) }\n",
    "\n",
    "# Fine-Tune regularization parameters\n",
    "\n",
    "tune_round3_lgb \\<- function(base_params, dtrain) { tune_grid \\<-\n",
    "expand.grid( lambda_l1 = c(0.01, 0.1, 1), lambda_l2 = c(0.01, 0.1, 1),\n",
    "min_gain_to_split = c(0.01, 0.1, 0.5))\n",
    "\n",
    "best_params \\<- base_params best_rmse \\<- Inf best_nround \\<- 0\n",
    "\n",
    "for (i in 1:nrow(tune_grid)) { current_params \\<- base_params\n",
    "current_params$lambda_l1 <- tune_grid$lambda_l1\\[i\\]\n",
    "current_params$lambda_l2 <- tune_grid$lambda_l2\\[i\\]\n",
    "current_params$min_gain_to_split <- tune_grid$min_gain_to_split\\[i\\]\n",
    "result \\<- evaluate_params_lgb(current_params, dtrain) if\n",
    "(result$rmse < best_rmse) {  best_rmse <- result$rmse best_params \\<-\n",
    "current_params best_nround \\<- result\\$nrounds } }\n",
    "\n",
    "return(list(params = best_params, nrounds = best_nround, rmse =\n",
    "best_rmse)) }\n",
    "\n",
    "# Fine-tune the number of trees\n",
    "\n",
    "fine_tune_trees_lgb \\<- function(params, dtrain, current_nround) {\n",
    "test_rounds \\<- seq(max(1, current_nround - 200), current_nround + 200,\n",
    "by = 50) best_rmse \\<- Inf best_nround \\<- current_nround\n",
    "\n",
    "for (n in test_rounds) { cv_results \\<- lgb.cv( params = params, data =\n",
    "dtrain, nrounds = n, nfold = 5, early_stopping_rounds = 50, verbose =\n",
    "-1)\n",
    "\n",
    "    current_rmse <- min(cv_results$evaluation_log$valid_rmse)\n",
    "    if (current_rmse < best_rmse) {\n",
    "      best_rmse <- current_rmse\n",
    "      best_nround <- n } }\n",
    "\n",
    "# Ensure final result is in the expected list format\n",
    "\n",
    "return(list(params = params, nrounds = best_nround, rmse = best_rmse)) }\n",
    "\n",
    "# Training the final model with the best parameters\n",
    "\n",
    "final_results_lgb \\<- fine_tune_trees_lgb(params = base_params_lgb,\n",
    "dtrain = dtrain_lgb, current_nround = 2000)\n",
    "\n",
    "lightgbm_model \\<- lgb.train( params =\n",
    "final_results_lgb$params, # Using best params from tuning  data = dtrain_lgb,  nrounds = final_results_lgb$nrounds,\n",
    "valids = list(test = dtest_lgb), verbose = -1)\n",
    "\n",
    "# End time measure\n",
    "\n",
    "toc()\n",
    "\n",
    "# Save the LightGBM model\n",
    "\n",
    "save(lightgbm_model, file = “D:/mymodels/lightgbm_model.RData”) \\# Load\n",
    "the saved LightGBM model #load(“D:/mymodels/lightgbm_model.RData”)\n",
    "\n",
    "\n",
    "    **Best hyperparameters and Predictors**\n",
    "\n",
    "    ```{r}\n",
    "    # Displaying Final Model Details for LightGBM\n",
    "    #cat(\"LightGBM Model Summary:\\n\")\n",
    "    #cat(\"Best Number of Trees:\", final_results_lgb$nrounds, \"\\n\")  # Best number of trees\n",
    "    #cat(\"Best Parameters:\\n\")\n",
    "\n",
    "    # Limiting the number of decimal places to 2 for the parameters\n",
    "    #print(lapply(final_results_lgb$params, function(x) if (is.numeric(x)) round(x, 2) else x))  # Display best hyperparameters rounded to 2 decimal places\n",
    "\n",
    "    # Variable Importance\n",
    "    importance <- lgb.importance(lightgbm_model)  # Get feature importance from the LightGBM model\n",
    "\n",
    "    # Limiting the importance values to 2 decimal places\n",
    "    importance$Gain <- round(importance$Gain, 2)\n",
    "    importance$Cover <- round(importance$Cover, 2)\n",
    "    importance$Frequency <- round(importance$Frequency, 2)\n",
    "\n",
    "    # Create an interactive table to display all the predictors and their importance values\n",
    "    datatable(importance, options = list(pageLength = 10), \n",
    "              caption = \"LightGBM Feature Importance\") %>%\n",
    "      formatPercentage('Gain', 2) %>%  # Format the 'Gain' values as percentages with 2 decimal places\n",
    "      formatPercentage('Cover', 2) %>%  # Format the 'Cover' values as percentages with 2 decimal places\n",
    "      formatPercentage('Frequency', 2)  # Format the 'Frequency' values as percentages with 2 decimal places\n",
    "\n",
    "    # Normalize the feature importances to a scale of 0 to 1 for the Gain column\n",
    "    importance$NormalizedGain <- importance$Gain / max(importance$Gain)\n",
    "\n",
    "    # Select the top 10 most important features based on the normalized Gain\n",
    "    top_10_importance <- importance[order(-NormalizedGain)][1:10]\n",
    "\n",
    "    # Plot Variable Importance for the top 10 features\n",
    "    ggplot(top_10_importance, aes(x = reorder(Feature, NormalizedGain), y = NormalizedGain)) +\n",
    "      geom_bar(stat = \"identity\", fill = \"lightblue\") +\n",
    "      coord_flip() +\n",
    "      labs(title = \"LightGBM Top 10 Most Important Features (Normalized)\", \n",
    "           x = \"Features\", \n",
    "           y = \"Normalized Gain\") +\n",
    "      theme_minimal() +\n",
    "      theme(panel.grid.major.y = element_blank(),  # Remove horizontal grid lines\n",
    "            panel.grid.minor.y = element_blank(),  # Remove minor horizontal grid lines\n",
    "            axis.text.x = element_text(angle = 0))  # Ensure x-axis labels are horizontal\n",
    "\n",
    "The LightGBM model was trained with 2000 boosting iterations, with\n",
    "binary classification as the objective and binary error as the\n",
    "evaluation metric. The best hyperparameters include boosting type “gbdt”\n",
    "and 4 threads for parallelization.\n",
    "\n",
    "Feature importance analysis shows that EXT_SOURCE_2 and EXT_SOURCE_3 are\n",
    "the most influential variables, with contributions of 20% and 19%,\n",
    "respectively. These features also have the highest cover, indicating\n",
    "that they affect a significant portion of the data, and are frequently\n",
    "used in decision tree splits. Other important features, such as\n",
    "EXT_SOURCE_1, DAYS_EMPLOYED_YEARS, and DAYS_BIRTH, also contribute to\n",
    "the model’s predictions but to a lesser extent.\n",
    "\n",
    "For this model, only AMT_ANNUITY, DAYS_LAST_PHONE_CHANGE, and\n",
    "NAME_EDUCATION_TYPE_higher_education are not among the top 10 most\n",
    "important features in the RFE.\n",
    "\n",
    "**Train Set Metrics**\n",
    "\n",
    "``` {r}\n",
    "# Predict probabilities on the train dataset\n",
    "pred_train_lgb <- predict(lightgbm_model, newdata = as.matrix(dt_train[, -which(names(dt_train) == \"TARGET\")]), type = \"response\")\n",
    "\n",
    "# ROC and AUC for the train set\n",
    "roc_train_lgb <- roc(dt_train$TARGET, pred_train_lgb, levels = c(0, 1), direction = \"<\")\n",
    "auc_train_lgb <- auc(roc_train_lgb)  # Calculate AUC\n",
    "cat(\"AUC Train set - LightGBM:\", round(auc_train_lgb, 2), \"\\n\")\n",
    "\n",
    "# Predict class labels for the train set using LightGBM model\n",
    "predictClassesTrainLGB <- ifelse(pred_train_lgb > 0.5, 1, 0)\n",
    "\n",
    "# Calculate Accuracy for the train set\n",
    "accuracy_train_lgb <- mean(predictClassesTrainLGB == dt_train$TARGET)\n",
    "cat(\"Accuracy Train set - LightGBM:\", round(accuracy_train_lgb, 2), \"\\n\")\n",
    "\n",
    "# Generate confusion matrix for the LightGBM model on the train set \n",
    "conf_matrix_train_lgb <- confusionMatrix(data = factor(predictClassesTrainLGB, levels = c(0, 1)),\n",
    "                                         reference = factor(dt_train$TARGET, levels = c(0, 1)),\n",
    "                                         positive = \"1\")  # \"1\" as the positive class\n",
    "```\n",
    "\n",
    "**AUC - LightGBM Model**\n",
    "\n",
    "``` {r}\n",
    "# Predict class probabilities on the test dataset using LightGBM\n",
    "pred_test_lgb <- predict(lightgbm_model, newdata = as.matrix(dt_test[, -which(names(dt_test) == \"TARGET\")]))  # Generate predicted probabilities for each class\n",
    "\n",
    "# For binary classification, probabilities for Class 1\n",
    "pred_test_lgb_class1 <- pred_test_lgb  # Probability for Class 1\n",
    "pred_test_lgb_class0 <- 1 - pred_test_lgb_class1  # Probability for Class 0\n",
    "\n",
    "# Calculate ROC and AUC for the test set\n",
    "roc_test_lgb <- roc(dt_test$TARGET, pred_test_lgb_class1, levels = c(0, 1), direction = \"<\")  # ROC object for LightGBM\n",
    "auc_test_lgb <- auc(roc_test_lgb)  # Calculate AUC\n",
    "cat(\"AUC Test set - LightGBM:\", round(auc_test_lgb, 2), \"\\n\")\n",
    "\n",
    "# Plot the ROC curve\n",
    "plot(roc_test_lgb, col = \"blue\", lwd = 2, main = \"ROC Curve - LightGBM Model\")\n",
    "```\n",
    "\n",
    "**Confusion Matrix - LightGBM Model**\n",
    "\n",
    "``` {r}\n",
    "# Convert probabilities to class labels (threshold 0.5 for binary classification)\n",
    "pred_test_lgb_class_labels <- ifelse(pred_test_lgb_class1 > 0.5, 1, 0)\n",
    "\n",
    "# Convert predicted labels and true labels to factors with the same levels\n",
    "pred_test_lgb_class_labels <- factor(pred_test_lgb_class_labels, levels = c(0, 1))\n",
    "true_labels <- factor(dt_test$TARGET, levels = c(0, 1))\n",
    "\n",
    "# Calculate the confusion matrix to evaluate the LightGBM model's performance\n",
    "conf_matrix_test_lgb <- confusionMatrix(data = pred_test_lgb_class_labels,   # Predicted class labels\n",
    "                                        reference = true_labels,           # True class labels from the test dataset\n",
    "                                        positive = \"1\")                    # Specify the positive class as \"Class1\"\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(conf_matrix_test_lgb)\n",
    "```\n",
    "\n",
    "Results for the LightGBM model (test set) include an AUC of 0.75,\n",
    "accuracy of 73%, sensitivity (true positive for Class 1) of 43%, and\n",
    "specificity (true negative for Class 0) of 88%. The LightGBM model\n",
    "performs almost identically to the XGBoost model, with both models\n",
    "achieving the same AUC and similar accuracy and specificity. While the\n",
    "sensitivity of LightGBM is slightly lower than that of XGBoost, both\n",
    "models effectively balance the identification of defaulters and\n",
    "non-defaulters.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### 6. Comparison Metrics\n",
    "\n",
    "**Final Metrics**\n",
    "\n",
    "The metrics below are simply to summarize what has already been\n",
    "interpreted and provide a unique comparative visualization of the\n",
    "models’ performance on both the training and test sets.\n",
    "\n",
    "``` {r}\n",
    "# Metrics summary with updated column names and descriptions\n",
    "metrics_df <- data.frame(\n",
    "  Model = c(\"Logistic Regression\\n(Train)\", \"Logistic Regression\\n(Test)\",\n",
    "            \"Random Forest\\n(Train)\", \"Random Forest\\n(Test)\",\n",
    "            \"XGBoost\\n(Train)\", \"XGBoost\\n(Test)\",\n",
    "            \"LightGBM\\n(Train)\", \"LightGBM\\n(Test)\"),\n",
    "  AUC = c(auc_train_logistic, auc_test_logistic,\n",
    "          auc_train_rf, auc_test_rf,\n",
    "          auc_train_xgb, auc_test_xgb,\n",
    "          auc_train_lgb, auc_test_lgb),\n",
    "  Accuracy = c(conf_matrix_train_logistic$overall[\"Accuracy\"], conf_matrix_test_logistic$overall[\"Accuracy\"],\n",
    "               conf_matrix_train_rf$overall[\"Accuracy\"], conf_matrix_test_rf$overall[\"Accuracy\"],\n",
    "               conf_matrix_train_xgb$overall[\"Accuracy\"], conf_matrix_test_xgb$overall[\"Accuracy\"],\n",
    "               conf_matrix_train_lgb$overall[\"Accuracy\"], conf_matrix_test_lgb$overall[\"Accuracy\"]),\n",
    "  Sensitivity_True_Positive_Rate_for_Class_1 = c(conf_matrix_train_logistic$byClass[\"Sensitivity\"], \n",
    "                                                 conf_matrix_test_logistic$byClass[\"Sensitivity\"],\n",
    "                                                 conf_matrix_train_rf$byClass[\"Sensitivity\"], \n",
    "                                                 conf_matrix_test_rf$byClass[\"Sensitivity\"],\n",
    "                                                 conf_matrix_train_xgb$byClass[\"Sensitivity\"], \n",
    "                                                 conf_matrix_test_xgb$byClass[\"Sensitivity\"],\n",
    "                                                 conf_matrix_train_lgb$byClass[\"Sensitivity\"], \n",
    "                                                 conf_matrix_test_lgb$byClass[\"Sensitivity\"]),\n",
    "  Specificity_True_Negative_Rate_for_Class_0 = c(conf_matrix_train_logistic$byClass[\"Specificity\"], \n",
    "                                                 conf_matrix_test_logistic$byClass[\"Specificity\"],\n",
    "                                                 conf_matrix_train_rf$byClass[\"Specificity\"], \n",
    "                                                 conf_matrix_test_rf$byClass[\"Specificity\"],\n",
    "                                                 conf_matrix_train_xgb$byClass[\"Specificity\"], \n",
    "                                                 conf_matrix_test_xgb$byClass[\"Specificity\"],\n",
    "                                                 conf_matrix_train_lgb$byClass[\"Specificity\"], \n",
    "                                                 conf_matrix_test_lgb$byClass[\"Specificity\"]))\n",
    "\n",
    "# Display the table with kable, enabling LaTeX to interpret the line breaks\n",
    "kable(metrics_df, \n",
    "      caption = \"Model Performance Metrics\", \n",
    "      digits = 2, \n",
    "      col.names = c(\"Model\", \"AUC\", \"Accuracy\", \n",
    "                    \"Sensitivity\\nTPR_1\", \"Specificity\\nTNR_0\"), \n",
    "      escape = FALSE)\n",
    "```\n",
    "\n",
    "The AUC represents the overall performance of the model in\n",
    "distinguishing between classes, measuring its ability to rank positive\n",
    "instances higher than negative ones across all possible thresholds.\n",
    "Accuracy indicates the proportion of correctly predicted instances among\n",
    "all instances.\n",
    "\n",
    "Sensitivity (TPR_1) represents the percentage of correct predictions\n",
    "among all actual class 1 instances, which refers to individuals who\n",
    "defaulted on their loans (TARGET=1). Specificity (TNR_0) represents the\n",
    "percentage of correct predictions among all actual class 0 instances,\n",
    "which refers to individuals who paid their loans on time (TARGET=0).\n",
    "\n",
    "#### 6.1 Bussiness relevance of each model\n",
    "\n",
    "**Logistic Regression**  \n",
    "It was straightforward but performed poorly in predicting defaulters,\n",
    "capturing only 37% of them, which was insufficient for credit\n",
    "prediction.\n",
    "\n",
    "**Random Forest**  \n",
    "It was effective at identifying non-defaulters (92% specificity),\n",
    "reducing the risk of lending to them. However, it missed 32% of\n",
    "defaulters, leading to more false negatives. Key predictors, such as\n",
    "EXT_SOURCE_1, EXT_SOURCE_2, and EXT_SOURCE_3, were significant, but its\n",
    "inability to handle complex relationships limited its effectiveness\n",
    "compared to other models.\n",
    "\n",
    "**XGBoost**  \n",
    "It offered the best performance with an AUC of 0.75. It excelled at\n",
    "capturing complex relationships, particularly between EXT_SOURCE_3 and\n",
    "EXT_SOURCE_2, which were crucial for predicting defaults. XGBoost\n",
    "effectively balanced sensitivity and specificity, improving the accuracy\n",
    "of credit decisions.\n",
    "\n",
    "**LightGBM**  \n",
    "It showed similar results to XGBoost, with key predictors like\n",
    "EXT_SOURCE_2 and EXT_SOURCE_3 playing a significant role in the decision\n",
    "process.\n",
    "\n",
    "#### 6.2 Ensemble Model for Better Credit Allocation\n",
    "\n",
    "Given this scenario, we propose adopting a combined model (Ensemble)\n",
    "that integrates predictions from all four models. Averaging the\n",
    "probabilities from each model has shown slightly better performance\n",
    "compared to individual models. The ensemble will balance the strengths\n",
    "of each technique—for example, the interpretability of logistic\n",
    "regression, the ability of Random Forest to capture non-linear patterns,\n",
    "and the accuracy of XGBoost and LightGBM. This approach provides a more\n",
    "robust and reliable solution for credit allocation, crucial for\n",
    "achieving financial inclusion without compromising Home Credit’s\n",
    "financial stability.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### 7. Predicting the Kaggle Test Set\n",
    "\n",
    "#### 7.1 Application Test Transformations\n",
    "\n",
    "The code chunks below aim to apply the same feature engineering to the\n",
    "target prediction file (application_test) as was performed on the\n",
    "application_train file. This ensures that the models created can make\n",
    "accurate predictions.\n",
    "\n",
    "\\`\\`\\`{r test transformation 1} \\# Create the clean data set\n",
    "app_test_clean \\<- application_test\n",
    "\n",
    "# Clean and transform\n",
    "\n",
    "app_test_clean \\<- app_test_clean %\\>% filter(AMT_INCOME_TOTAL \\<=\n",
    "5000000)\n",
    "\n",
    "\n",
    "    ```{r test transformation 2}\n",
    "    # Transformation for better understanding ages\n",
    "    app_test_clean$DAYS_BIRTH <- app_test_clean$DAYS_BIRTH / -365\n",
    "\n",
    "\\`\\`\\`{r test transformation 3} \\# Create a new column app_test_clean\n",
    "\\<- application_test %\\>% mutate(DAYS_EMPLOYED_ANOM =\n",
    "ifelse(DAYS_EMPLOYED == 365243, TRUE, FALSE))\n",
    "\n",
    "# Replace the anomalies for NA\n",
    "\n",
    "app_test_clean$DAYS_EMPLOYED[app_test_clean$DAYS_EMPLOYED == 365243\\]\n",
    "\\<- NA\n",
    "\n",
    "# Convert DAYS_EMPLOYED to years\n",
    "\n",
    "app_test_clean \\<- app_test_clean %\\>% mutate(DAYS_EMPLOYED_YEARS =\n",
    "DAYS_EMPLOYED / -365)\n",
    "\n",
    "# Remove DAYS_EMPLOYED\n",
    "\n",
    "app_test_clean \\<- app_test_clean %\\>% select(-DAYS_EMPLOYED)\n",
    "\n",
    "\n",
    "    ```{r test transformation 4}\n",
    "    # Loop through each column of the dataset\n",
    "    for (col in names(app_test_clean)) {\n",
    "      # Check if the column is of character type\n",
    "      if (is.character(app_test_clean[[col]])) {\n",
    "        # Replace empty strings with \"Unknown\"\n",
    "        app_test_clean[[col]] <- as.character(app_test_clean[[col]])  # Ensure column is character\n",
    "        app_test_clean[[col]][app_test_clean[[col]] == \"\"] <- \"Unknown\"  # Replace empty strings\n",
    "        app_test_clean[[col]] <- as.factor(app_test_clean[[col]]) }}  # Convert back to factor if needed\n",
    "\n",
    "\\`\\`\\`{r test transformation 5} \\# Convert OCCUPATION_TYPE to factor if\n",
    "not already done\n",
    "app_test_clean$OCCUPATION_TYPE <- as.factor(app_test_clean$OCCUPATION_TYPE)\n",
    "\n",
    "# Replace empty strings with “Unknown”\n",
    "\n",
    "app_test_clean$OCCUPATION_TYPE <- as.character(app_test_clean$OCCUPATION_TYPE)\n",
    "\\# Convert to character\n",
    "app_test_clean$OCCUPATION_TYPE[app_test_clean$OCCUPATION_TYPE == ““\\]\n",
    "\\<-”Unknown” \\# Replace empty strings\n",
    "app_test_clean$OCCUPATION_TYPE <- as.factor(app_test_clean$OCCUPATION_TYPE)\n",
    "\\# Convert back to factor\n",
    "\n",
    "\n",
    "    ```{r test transformation 6, echo= FALSE}\n",
    "    # Filter out instances with 'XNA' in CODE_GENDER and remove unused levels\n",
    "    app_test_clean <- app_test_clean[app_test_clean$CODE_GENDER != \"XNA\", ]\n",
    "    app_test_clean$CODE_GENDER <- droplevels(app_test_clean$CODE_GENDER)\n",
    "\n",
    "    # Convert the DAYS_EMPLOYED_ANOM variable to a factor\n",
    "    app_test_clean$DAYS_EMPLOYED_ANOM <- as.factor(app_test_clean$DAYS_EMPLOYED_ANOM)\n",
    "\n",
    "    # Identify all non-numeric columns in the dataset (factors)\n",
    "    non_numeric_list <- unlist(lapply(app_test_clean, is.factor))\n",
    "\n",
    "    # Create a data table with only non-numeric columns\n",
    "    data_non_num <- setDT(app_test_clean)[,..non_numeric_list]\n",
    "\n",
    "    # Function to clean the levels of each factor variable\n",
    "    clean_factor_levels <- function(factor_var) {\n",
    "      # Replace spaces and punctuation with underscores and convert to lowercase\n",
    "      levels(factor_var) <- tolower(gsub(\"[[:space:][:punct:]]\", \"_\", levels(factor_var)))\n",
    "      return(factor_var)}\n",
    "\n",
    "    # Applying the function to each variable in the data.table\n",
    "    data_non_num[] <- lapply(data_non_num, function(x) {\n",
    "      # Check if the variable is a factor\n",
    "      if (is.factor(x)) {\n",
    "        # Clean the factor levels\n",
    "        clean_factor_levels(x)\n",
    "      } else {\n",
    "        # Return the variable unchanged if it is not a factor\n",
    "        return(x) }})\n",
    "      \n",
    "    # Create dummy variables for all non-numeric variables, dropping the second level to avoid multicollinearity\n",
    "    dummies <- dummyVars(~ ., data = data_non_num, drop2nd = TRUE)\n",
    "\n",
    "    # Apply the transformation to generate the dummy variables\n",
    "    data_non_num_dum <- predict(dummies, newdata = data_non_num)\n",
    "\n",
    "    # Convert the result to a data frame\n",
    "    data_non_num_dum <- as.data.frame(data_non_num_dum)\n",
    "\n",
    "    # Replace special characters with underscores in column names\n",
    "    #colnames(data_non_num_dum) <- gsub(\"[/\\\\.]\", \"_\", colnames(data_non_num_dum))\n",
    "\n",
    "\\`\\`\\`{r test transformation 7} #Function to change index to column\n",
    "index_to_col \\<- function(data, Column_Name){ data \\<- cbind(newColName\n",
    "= rownames(data), data) rownames(data) \\<- 1:nrow(data)\n",
    "colnames(data)\\[1\\] \\<- Column_Name return (data) }\n",
    "\n",
    "# Recreate the list that includes numeric and integer columns\n",
    "\n",
    "numeric_integer_list \\<- unlist(lapply(app_test_clean, function(x)\n",
    "is.numeric(x) \\|\\| is.integer(x)))\n",
    "\n",
    "# Create a new data frame with the numeric and integer columns\n",
    "\n",
    "data_num \\<- setDT(app_test_clean)\\[, ..numeric_integer_list\\] \\# Select\n",
    "only numeric and integer columns\n",
    "\n",
    "# Combine one-hot encoded data with numeric and integer data\n",
    "\n",
    "test_pre \\<- cbind(data_non_num_dum, data_num)\n",
    "\n",
    "\n",
    "    ```{r test transformation 8}\n",
    "    # Calculate the percentage of missing values for each column\n",
    "    mv <- as.data.frame(apply(test_pre, 2, function(col) sum(is.na(col)) / length(col)))\n",
    "    colnames(mv)[1] <- \"missing_values\"  # Rename the first column to \"missing_values\"\n",
    "\n",
    "    # Add a column with the index as the first column\n",
    "    mv <- index_to_col(mv, 'Column')\n",
    "\n",
    "    # Order the missing values in descending order\n",
    "    mv <- setDT(mv)[order(missing_values, decreasing = TRUE)]\n",
    "\n",
    "    # Fill in the missing values using aggregation\n",
    "    test_pre <- na.aggregate(test_pre)\n",
    "\n",
    "    # Replace dots with underscores\n",
    "    colnames(test_pre) <- gsub(\"\\\\.\", \"_\", colnames(test_pre))\n",
    "\n",
    "    # Replace spaces, special characters, or dashes in column names with underscores\n",
    "    #colnames(test_pre) <- gsub(\"[[:punct:]\\\\s-]+\", \"_\", colnames(test_pre))\n",
    "\n",
    "\\`\\`\\`{r test transformation 9} \\# Get the names of all columns in\n",
    "data_pre_sample except for TARGET cols_to_keep_test \\<-\n",
    "colnames(data_pre_sample)\\[colnames(data_pre_sample) != “TARGET”\\]\n",
    "\n",
    "# Create a new data frame that only includes the selected columns\n",
    "\n",
    "test_pre \\<- as.data.frame(test_pre)\\[, (colnames(test_pre) %in%\n",
    "cols_to_keep_test)\\]\n",
    "\n",
    "# Using the same test set name\n",
    "\n",
    "dt_test_kaggle \\<-test_pre\n",
    "\n",
    "\n",
    "    #### 7.2 Predicting Application Test\n",
    "\n",
    "    Below, we will make probability predictions for each model for submission.\n",
    "\n",
    "    **Logistic Regression**\n",
    "\n",
    "    ```{r, message=FALSE, warning=FALSE}\n",
    "    # Predict probabilities on the test dataset using Logistic Regression\n",
    "    pred_logistic <- predict(logistic_model, newdata = dt_test_kaggle, type = \"prob\") \n",
    "\n",
    "**Random Forest**\n",
    "\n",
    "`{r, message=FALSE, warning=FALSE} # Predict probabilities on the test dataset using Random Forest pred_random_forest <- predict(random_forest_model, newdata = dt_test_kaggle, type = \"prob\")`\n",
    "\n",
    "**XGBoost Model**\n",
    "\n",
    "\\`\\`\\`{r, message=FALSE, warning=FALSE} \\# Convert to matrix format\n",
    "(excluding any columns that are not features) dt_test_kaggle_matrix \\<-\n",
    "as.matrix(dt_test_kaggle)\n",
    "\n",
    "# Predict class probabilities for the test dataset using XGBoost\n",
    "\n",
    "pred_xgboost \\<- predict(xgboost_model, newdata = dt_test_kaggle_matrix,\n",
    "type = “prob”)\n",
    "\n",
    "# Extract probabilities\n",
    "\n",
    "pred_xgboost_class1 \\<- pred_xgboost \\# Probabilities for Class 1\n",
    "pred_xgboost_class0 \\<- 1 - pred_xgboost_class1 \\# Probabilities for\n",
    "Class 0 (1 - prob_class1)\n",
    "\n",
    "\n",
    "    **LightGBM Model**\n",
    "\n",
    "    ```{r, message=FALSE, warning=FALSE}\n",
    "    # Predict class probabilities for the test dataset using LightGBM\n",
    "    pred_lgb <- predict(lightgbm_model, newdata = dt_test_kaggle_matrix)\n",
    "\n",
    "    # Extract probabilities for Class 1\n",
    "    pred_lgb_class1 <- pred_lgb  # Probabilities for Class 1\n",
    "    pred_lgb_class0 <- 1 - pred_lgb_class1  # Probabilities for Class 0 (1 - prob_class1)\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### 8. Submission and Kaggle Score\n",
    "\n",
    "**Average of the models**\n",
    "\n",
    "This submission combines the predictions using the ensemble method,\n",
    "calculating the average of all predictions for submission to Kaggle\n",
    "leveraging the strengths of each individual model to enhance overall\n",
    "performance.\n",
    "\n",
    "``` {r}\n",
    "# Initialize the data frame with the SK_ID_CURR column from application_test\n",
    "results_kaggle <- data.frame(SK_ID_CURR = application_test$SK_ID_CURR)\n",
    "\n",
    "# Logistic Regression returns probabilities in a data frame with columns Class0 and Class1\n",
    "results_kaggle$TARGET_Logistic <- pred_logistic$Class1  # Use Class1 probabilities for TARGET\n",
    "\n",
    "# Random Forest returns probabilities in a data frame with columns Class0 and Class1\n",
    "results_kaggle$TARGET_RF <- pred_random_forest$Class1  # Add Class1 probabilities for RF\n",
    "\n",
    "# XGBoost returns a numeric vector of probabilities for Class1\n",
    "results_kaggle$TARGET_XGBoost <- pred_xgboost  # Directly use probabilities for Class1\n",
    "\n",
    "# LightGBM returns a named numeric vector, where \"1\" corresponds to Class1 probabilities\n",
    "results_kaggle$TARGET_LGB <- pred_lgb  # Use Class1 probabilities for LGB (probabilities for class \"1\")\n",
    "\n",
    "# Calculate the average probabilities for \"Class1\"\n",
    "results_kaggle$TARGET_Average <- rowMeans(results_kaggle[, c(\"TARGET_Logistic\", \"TARGET_RF\", \"TARGET_XGBoost\", \"TARGET_LGB\")], na.rm = TRUE)\n",
    "\n",
    "# Create the final submission data frame with only SK_ID_CURR and average TARGET\n",
    "submission_average <- results_kaggle[, c(\"SK_ID_CURR\", \"TARGET_Average\")]\n",
    "colnames(submission_average) <- c(\"SK_ID_CURR\", \"TARGET\")  # Rename columns to match submission format\n",
    "\n",
    "# Save the final submission data frame as CSV\n",
    "write.csv(submission_average, file = \"D:/mymodels/submission_average.csv\", row.names = FALSE, quote = FALSE)\n",
    "```\n",
    "\n",
    "**Kaggle Score**\n",
    "\n",
    "The score achieved by averaging the results of all the models was 0.739\n",
    "only 8% lower than the competition winner. The individual scores for\n",
    "each model were as follows:\n",
    "\n",
    "Logistic Regression: 0.730\n",
    "\n",
    "Random Forest: 0.729\n",
    "\n",
    "XGBoost: 0.733\n",
    "\n",
    "LightGBM: 0.733\n",
    "\n",
    "``` {r}\n",
    "knitr::include_graphics(\"Kaggle Kleyton final.jpg\")\n",
    "```\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "Predicting defaults is inherently challenging, as all models rely\n",
    "heavily on external credit scoring data. However, integrating additional\n",
    "variables such as AMT_CREDIT, AMT_GOODS_PRICE, DAYS_EMPLOYED_YEARS,\n",
    "DAYS_BIRTH, DAYS_ID_PUBLISH, OWN_CAR_AGE, DAYS_LAST_PHONE_CHANGE, and\n",
    "NAME_EDUCATION_TYPE_higher_education can significantly enhance model\n",
    "performance. These insights enable the company to focus on the most\n",
    "relevant factors, reducing the need for unnecessary customer inputs.\n",
    "\n",
    "By adopting an ensemble model that combines predictions from all four\n",
    "approaches, Home Credit can leverage the strengths of each technique to\n",
    "create a more balanced and accurate system. This approach ensures better\n",
    "credit allocation decisions, reduces the risk of defaults, and fosters\n",
    "financial inclusion while safeguarding the company’s financial\n",
    "stability."
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
